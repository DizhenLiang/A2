{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f729fa",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "\n",
    "Embedded Documents: Use embedded documents to store related data together, which allows for efficient querying.\n",
    "\n",
    "The common attribute of these two dataset is date, if we do embedded data model, it would accelerate all the queries related with date, range of hotspot data would be limited to the range with this specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/student/A2/drive-download-20240507T073745Z-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb05b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hotspot = pd.read_csv(\"./hotspot_historic.csv\", sep=',')\n",
    "climate = pd.read_csv(\"./climate_historic.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspot.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b4c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f50ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(climate['date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b66638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict(filename):\n",
    "    with open(filename, mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "        #DictReader read the first row as column headers\n",
    "        #return an iterator that produce disctionaries for each row\n",
    "        #keys: column headers\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        #convert iterator into list of dictionaries\n",
    "        #keys: columne headers, values: cell values\n",
    "        return list(reader)\n",
    "\n",
    "\n",
    "# Read climate data\n",
    "climate_data = csv_to_dict('climate_historic.csv')\n",
    "\n",
    "# Read hotspot data\n",
    "hotspot_data = csv_to_dict('hotspot_historic.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "#Residence wifi ip address: 172.16.62.212\n",
    "ipadd = \"172.16.33.120\"\n",
    "\n",
    "\n",
    "client = MongoClient(ipadd, 27017)\n",
    "db = client.fit3182_db\n",
    "collection = db.fit3182_assignment_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avoid error from exsiting collection with the same name\n",
    "#collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime objects for proper comparison\n",
    "                        #convert from d/m/Y format into \n",
    "# climate['date'] = pd.to_datetime(climate['date'], format='%d/%m/%Y')\n",
    "# hotspot['date'] = pd.to_datetime(hotspot['date'], format='%d/%m/%Y')\n",
    "\n",
    "#since the locaiton of the sensors would not change across the dates\n",
    "#since the station would not change location across the dates\n",
    "# Embed fire data into climate data\n",
    "# loop through each entry of the climate \n",
    "for index, climate_row in climate.iterrows():\n",
    "    \n",
    "    #filter out the climate data entries that have the sensor data\n",
    "    #since they are important, others are not\n",
    "    #since stations have different data according to dates\n",
    "\n",
    "    #for each climate, filter out the hotspot/sensors with matching/corresponding date\n",
    "    #list of true/false to filter out the hotspots related to the dates\n",
    "    useful_dates = hotspot['date'] == climate_row['date']\n",
    "    matching_fires = hotspot[useful_dates]\n",
    "\n",
    "    #convert to list of disctionaries\n",
    "    fires_list = matching_fires.to_dict('records')\n",
    "    \n",
    "    #inner-join\n",
    "#     if fires_list:\n",
    "        #print(fires_list)\n",
    "    #left-outer join\n",
    "    climate_doc = {\n",
    "        'stations': climate_row['station'],\n",
    "                                    #string from time with format, \n",
    "                                    #since MongoDB can not store object\n",
    "        #.strftime('%d/%m/%Y')\n",
    "        'date': climate_row['date'],\n",
    "        'air_temperature_celcius': climate_row['air_temperature_celcius'],\n",
    "        'relative_humidity': climate_row['relative_humidity'],\n",
    "        'windspeed_knots': climate_row['windspeed_knots'],\n",
    "        'max_wind_speed': climate_row['max_wind_speed'],\n",
    "        'precipitation': climate_row['precipitation'],\n",
    "        'GHI_w/m2': climate_row['GHI_w/m2'],\n",
    "        #embed sensor data here\n",
    "        'hotspots': fires_list\n",
    "    }\n",
    "\n",
    "    collection.insert_one(climate_doc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0acff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "\"\"\"\n",
    "\n",
    "# Q2.a.\n",
    "res = collection.find(\n",
    "    {\n",
    "    'date': '12/12/2023'\n",
    "    }\n",
    ")\n",
    "\n",
    "for d in res:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb28596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question b\n",
    "\"\"\"\n",
    "\n",
    "# as long as include filtering/matching on the documents of the hotspots array field in the main document\n",
    "# use aggregate + pipeline\n",
    "\n",
    "pipeline = [\n",
    "    {   #first match, filter the station+date documents based on the hotspots that match\n",
    "        '$match': {\n",
    "            'hotspots.surface_temperature_celcius': {\n",
    "                '$gte': 65,\n",
    "                '$lte': 100\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {   # since not all sensors in the document indexed by date and station\n",
    "        # fulifll the condition, use unwind to release documents in array field(hotspots)\n",
    "        # to flatten array\n",
    "        '$unwind': '$hotspots'  # Unwind the hotspots array\n",
    "    },\n",
    "    {   #second match, filter the hotspot documents of the hotspots array\n",
    "        '$match': {\n",
    "            'hotspots.surface_temperature_celcius': {\n",
    "                '$gte': 65,\n",
    "                '$lte': 100\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': { #   #value from output document after matching\n",
    "                            #$ -> latitude field\n",
    "            'latitude': '$hotspots.latitude',\n",
    "            'longitude': '$hotspots.longitude',\n",
    "            'surface_temperature_celcius': '$hotspots.surface_temperature_celcius',\n",
    "            'confidence': '$hotspots.confidence'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "res = collection.aggregate(pipeline)\n",
    "\n",
    "# To display only the latitude\n",
    "for d in res:\n",
    "    pprint(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9757e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question c\n",
    "\"\"\"\n",
    "#filtering only on main document, no pipeline needed\n",
    "\n",
    "res = collection.find(\n",
    "    {\n",
    "        'date':{\n",
    "            '$in': ['15/12/2023', '16/12/2023']\n",
    "        }\n",
    "    },\n",
    "    {   #only work for main \n",
    "        '_id': 0,\n",
    "        'date': 1,\n",
    "        'air_temperature_celcius': 1,\n",
    "        'relative_humidity': 1,\n",
    "                                        #must be used for array field\n",
    "        'surface_temperature_celcius': '$hotspots.surface_temperature_celcius'\n",
    "    }\n",
    ")\n",
    "\n",
    "for d in res:\n",
    "    pprint(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c7947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question d\n",
    "\"\"\"\n",
    "\n",
    "pipeline = [\n",
    "    {\n",
    "        '$match': {\n",
    "            'hotspots.confidence': {\n",
    "                '$gte': 80,\n",
    "                '$lte': 100\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$unwind': '$hotspots'  # Corrected to a string\n",
    "    },\n",
    "    {\n",
    "        '$match': {\n",
    "            'hotspots.confidence': {\n",
    "                '$gte': 80,\n",
    "                '$lte': 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # {   # no attributes from main document can be shown with fields of document in array field\n",
    "    #     '$project': {\n",
    "    #         'datetime': '$hotspots.datetime',\n",
    "    #         'surface_temperature_celcius': '$hotspots.surface_temperature_celcius',\n",
    "    #         'confidence': '$hotspots.confidence'  \n",
    "    #     }\n",
    "    # }\n",
    "]\n",
    "\n",
    "res = collection.aggregate(pipeline)\n",
    "\n",
    "for d in res:\n",
    "    pprint(d)\n",
    "\n",
    "\n",
    "res = collection.aggregate(pipeline)\n",
    "\n",
    "for d in res:\n",
    "    pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question e\n",
    "\"\"\"\n",
    "                        #sort \n",
    "#res = collection.find().sort('surface_temperature_celcius', -1).limit(10)\n",
    "res = collection.find().sort('surface_temperature_celcius', pymongo.DESCENDING).limit(10)\n",
    "\n",
    "for i, d in enumerate(res):\n",
    "    # only 10 records for res\n",
    "    # if i == 9:\n",
    "    pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question f\n",
    "\"\"\"\n",
    "\n",
    "pipeline = [\n",
    "    {\n",
    "        '$group': {\n",
    "            '_id': '$date',  # group by the 'date' field\n",
    "            'count': {'$sum': 1}  # count the documents in each group\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "res = collection.aggregate(pipeline)\n",
    "for i, d in enumerate(res):\n",
    "    pprint(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb92f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question g\n",
    "\"\"\"\n",
    "\n",
    "#Fires records = fields from array field -> pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        '$unwind': '$hotspots'  # Unwind the hotspots array\n",
    "    },\n",
    "    {\n",
    "        '$match': {\n",
    "            'hotspots.confidence': {\n",
    "                '$lt': 70  # Match documents where confidence is less than 70\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project': {\n",
    "            #Just Fire records\n",
    "            '_id': 0,\n",
    "            'confidence': '$hotspots.confidence',  \n",
    "            'date': '$hotspots.date',  \n",
    "            'datetime': '$hotspots.datetime',  \n",
    "            'latitude': '$hotspots.latitude',  \n",
    "            'longitude': '$hotspots.longitude', \n",
    "            'surface_temperature_celcius': '$hotspots.surface_temperature_celcius'\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "res = collection.aggregate(pipeline)\n",
    "for i, d in enumerate(res):\n",
    "    pprint(d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78057d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question h\n",
    "\"\"\"\n",
    "\n",
    "pipeline = [\n",
    "    {\n",
    "        '$unwind': '$hotspots' \n",
    "    },\n",
    "    {\n",
    "        '$group': {\n",
    "            #_id must exist in $group can be replaced\n",
    "            '_id': '$hotspots.date',  # group by the date field from hotspots\n",
    "            'avg_surface_temperature': {\n",
    "                '$avg': '$hotspots.surface_temperature_celcius' \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {   #modify the project of the output after all above processing\n",
    "        '$project': {\n",
    "            '_id': 0,  # Exclude the _id field\n",
    "            'date': '$_id',  # Map the _id field (which is the date) to 'date' in the output\n",
    "            'avg_surface_temperature': 1  # \n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$sort': {'date': 1}  # Optional: sort by date\n",
    "    }\n",
    "]\n",
    "\n",
    "res = collection.aggregate(pipeline)\n",
    "for d in res:\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question i\n",
    "\"\"\"\n",
    "# sort all by field 'GHI_w/m2' count for 10, -1 decending\n",
    "res = collection.find().sort('GHI_w/m2', -1).limit(10)\n",
    "\n",
    "for d in res:\n",
    "    pprint(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30c1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question j\n",
    "\"\"\"\n",
    "#main document aggregate piepeline\n",
    "pipeline = [\n",
    "    {\n",
    "        '$match':{\n",
    "            'precipitation':{\n",
    "                '$gte': 0.2,\n",
    "                '$lte': 0.35\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        '$project':{   \n",
    "            '_id': 0,\n",
    "            'date': 1,\n",
    "            'precipitation': 1\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "res = collection.aggregate(pipeline)\n",
    "for d in res:\n",
    "    pprint(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part A\n",
    "Task 2. Querying MongoDBusingPyMongo\n",
    "Question 3\n",
    "\"\"\"\n",
    "\n",
    "# Assuming 'db' is your MongoDB database object and 'climate_data' is your collection.\n",
    "db.climate_data.create_index([\n",
    "    ('date', pymongo.ASCENDING),\n",
    "    ('station', pymongo.ASCENDING)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f9409",
   "metadata": {},
   "source": [
    "# since there is station for each day, with the compound index of date and station, we can efficiently access the data from climate, and the hotspots related to that date are limited, which can be further filtered with the locations of the hotspots by clutering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query the collection using the compound index\n",
    "\n",
    "# All documents for station 948700 on January 1, 2023\n",
    "results = collection.find({\n",
    "    'date': '31/12/2023',\n",
    "    'station': 948700\n",
    "})\n",
    "\n",
    "# Process the results\n",
    "for doc in results:\n",
    "    pprint(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b66241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part b\n",
    "Task 1. Processing Data Stream (45%)\n",
    "Question a\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#res = collection.find().sort('surface_temperature_celcius', pymongo.DESCENDING).limit(10)\n",
    "res = collection.find().sort('date', pymongo.DESCENDING).limit(1)\n",
    "\n",
    "for d in res:\n",
    "    res = d\n",
    "#     print(d)\n",
    "\n",
    "latest_date = res['date']\n",
    "#string to datetime object\n",
    "# latest_date = datetime.strptime(latest_date, '%Y-%m-%d %H:%M:%S')\n",
    "#string to datetime object\n",
    "latest_date = datetime.strptime(latest_date, '%d/%m/%Y')\n",
    "start_date = latest_date + timedelta(days=1)\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka3 import KafkaProducer\n",
    "import random\n",
    "import json\n",
    "\n",
    "hostip = \"172.16.55.43\" # change it to your IP\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    producer = None\n",
    "    \n",
    "    try:\n",
    "        producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'],api_version=(0,10))\n",
    "    \n",
    "    except Exception as ex:\n",
    "        print(\"Exception while connecting Kafka\")\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return producer \n",
    "    \n",
    "def publish_msg(producer, topic, data):\n",
    "    try:\n",
    "        #print(data)        #send the  value by dumps to JSON object\n",
    "                            #then encode with utf-8 into bytes\n",
    "        value_bytes = json.dumps(data).encode('utf-8')\n",
    "        producer.send(topic, value=value_bytes)\n",
    "        #print(data)\n",
    "        \"\"\"\n",
    "        The flush() method in Kafka’s producer API is used to ensure that all \n",
    "        previously sent messages have been transmitted to the server and acknowledged \n",
    "        before proceeding. When you call flush(), it blocks the current thread and waits \n",
    "        for the producer to complete the sending of all records\n",
    "        \"\"\"\n",
    "        producer.flush()  \n",
    "    except Exception as ex:\n",
    "        print('Exception from publish_msg func')\n",
    "        print(str(ex))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7bfc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Event Producer 1\n",
    "\"\"\"\n",
    "climate_stream_csv = pd.read_csv(\"./climate_streaming.csv\", sep=',')\n",
    "climate_stream_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd9ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main Function\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    topic = 'Scenaries01'\n",
    "    print('Publishing records..')\n",
    "    producer01 = connect_kafka_producer()\n",
    "\n",
    "\n",
    "    # Assuming 'climate' is your DataFrame\n",
    "    #print(len(climate.index))\n",
    "    rand_row_indices = list(range(len(climate.index)))\n",
    "    random.shuffle(rand_row_indices)  \n",
    "    #print(len(indices))\n",
    "\n",
    "    for index, climate_row in climate_stream_csv.iterrows():\n",
    "        climate_row = climate_stream_csv.loc[rand_row_indices[index]]\n",
    "\n",
    "        #inner-join\n",
    "    #     if fires_list:\n",
    "            #print(fires_list)\n",
    "\n",
    "        #left-outer join\n",
    "        #print(start_date.strftime('%Y-%m-%d'))\n",
    "        climate_doc = {\n",
    "                                        #string from time with format, \n",
    "                                        #since MongoDB can not store object\n",
    "            #.strftime('%d/%m/%Y')\n",
    "            #convert datetime object to string in format\n",
    "            # laster dumps not work on datetime object but number of string\n",
    "            'latitude': float(climate_row['latitude']),\n",
    "            'longtitude': float(climate_row['longitude']),\n",
    "            'air_temperature_celcius': int(climate_row['air_temperature_celcius']),\n",
    "            'relative_humidity': float(climate_row['relative_humidity']),\n",
    "            'windspeed_knots': float(climate_row['windspeed_knots']),\n",
    "            'max_wind_speed': float(climate_row['max_wind_speed']),\n",
    "            #'precipitation ' blank space count as char in csv column\n",
    "            # get to do try except, return None if column not exist\n",
    "            #'precipitation ': str(climate_row.get(['precipitation '])),\n",
    "            #'precipitation': str(climate_row.get(['precipitation'])),\n",
    "            'precipitation': str(climate_row['precipitation']),\n",
    "            'GHI_w/m2': int(climate_row['GHI_w/m2']),\n",
    "            'producer': \"Producer_1\",\n",
    "            'date': start_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        }\n",
    "        #update start date\n",
    "        #print(climate_doc['date'])\n",
    "        #producer publish msg in strictured streaming\n",
    "        # convert the dictionary to JSON-formatted string and encode as UFT-8 by publish_msg\n",
    "        # msg = json.dumps(climate_doc)\n",
    "        print(climate_doc)\n",
    "        publish_msg(producer01, topic, climate_doc)\n",
    "        start_date += timedelta(days=1)\n",
    "        # change to 1 for debugging\n",
    "        sleep(1)\n",
    "        #sleep(10) # every 10 seconds send\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Event Producer 2\n",
    "\"\"\"\n",
    "hotspot_AQUA = pd.read_csv(\"./hotspot_AQUA_streaming.csv\", sep=',')\n",
    "\n",
    "hotspot_AQUA.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main Function\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    topic = 'Scenaries02'\n",
    "    print('Publishing records..')\n",
    "    producer02 = connect_kafka_producer()\n",
    "    \n",
    "    \n",
    "    rand_row_indices = list(range(len(hotspot_AQUA.index)))\n",
    "    random.shuffle(rand_row_indices) \n",
    "    # random choose time from 0 to 10\n",
    "    max_time = 10\n",
    "    #rand_time = list(range(1,max_time))\n",
    "#     random.shuffle(rand_time) \n",
    "     \n",
    "\n",
    "    for index, hotspot_AQUA_row in hotspot_AQUA.iterrows():\n",
    "        hotspot_AQUA_row = hotspot_AQUA.loc[rand_row_indices[index]]\n",
    "\n",
    "        hotspot_AQUA_doc = {\n",
    "            'latitude': hotspot_AQUA_row['latitude'],\n",
    "            'longtitude': hotspot_AQUA_row['longitude'],\n",
    "            'confidence': hotspot_AQUA_row['confidence'],\n",
    "            'surface_temperature_celcius': hotspot_AQUA_row['surface_temperature_celcius'],\n",
    "            'producer': \"Producer_2\",\n",
    "            'random_time': rand_row_indices[index]%max_time + 1 # at least 1 sec\n",
    "        }\n",
    "        #update start date\n",
    "        #print(climate_doc['date'])\n",
    "        #producer publish msg in strictured streaming\n",
    "        # convert the dictionary to JSON-formatted string and encode as UFT-8 by publish_msg\n",
    "        # msg = json.dumps(climate_doc)\n",
    "        #print(climate_doc)\n",
    "        #print(hotspot_AQUA_doc['random_time'])\n",
    "        publish_msg(producer02, topic, hotspot_AQUA_doc)\n",
    "        # every random seconds\n",
    "        sleep(hotspot_AQUA_doc['random_time']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be657ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Event Producer 3\n",
    "\"\"\"\n",
    "hotspot_TERRA= pd.read_csv(\"./hotspot_TERRA_streaming.csv\", sep=',')\n",
    "\n",
    "hotspot_TERRA.head()\n",
    "\n",
    "\"\"\"\n",
    "Main Function\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    topic = 'Scenaries03'\n",
    "    print('Publishing records..')\n",
    "    producer03 = connect_kafka_producer()\n",
    "    \n",
    "    \n",
    "    rand_row_indices = list(range(len(hotspot_AQUA.index)))\n",
    "    random.shuffle(rand_row_indices) \n",
    "    # random choose time from 0 to 10\n",
    "    max_time = 10\n",
    "    #rand_time = list(range(1,max_time))\n",
    "#     random.shuffle(rand_time) \n",
    "     \n",
    "\n",
    "    for index, hotspot_TERRA_row in hotspot_TERRA.iterrows():\n",
    "        hotspot_TERRA_row = hotspot_TERRA.loc[rand_row_indices[index]]\n",
    "\n",
    "        hotspot_TERRA_doc = {\n",
    "            'latitude': hotspot_TERRA_row['latitude'],\n",
    "            'longtitude': hotspot_TERRA_row['longitude'],\n",
    "            'confidence': hotspot_TERRA_row['confidence'],\n",
    "            'surface_temperature_celcius': hotspot_TERRA_row['surface_temperature_celcius'],\n",
    "            'producer': \"Producer_3\",\n",
    "            'random_time': rand_row_indices[index]%max_time + 1 # at least 1 sec\n",
    "        }\n",
    "        #update start date\n",
    "        #print(climate_doc['date'])\n",
    "        #producer publish msg in strictured streaming\n",
    "        # convert the dictionary to JSON-formatted string and encode as UFT-8 by publish_msg\n",
    "        # msg = json.dumps(climate_doc)\n",
    "        #print(climate_doc)\n",
    "        print(hotspot_TERRA_doc['random_time'])\n",
    "        publish_msg(producer01, topic, hotspot_AQUA_doc)\n",
    "        # every random seconds\n",
    "        sleep(hotspot_TERRA_doc['random_time']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab0374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74424165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
