{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce48e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a70445b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "import geohash\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "#change ip address\n",
    "host_ip = \"10.192.33.112\"\n",
    "#172.16.33.120\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Streaming Application')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#append stream from multiple topic together\n",
    "topic = \"Producer1|Producer2|Producer3\"\n",
    "#topic_hotspot = 'A2_hotspot_topic'\n",
    "# topic_AQUA = 'Scenaries02'\n",
    "# topic_TERRA  = 'Scenaries03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e280884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribePattern', topic) #listen to multiple topics\n",
    "    .load()\n",
    ")\n",
    "kafka_sdf.printSchema()\n",
    "\n",
    "#select the value column in dataframe\n",
    "climatehotspot_sdf = kafka_sdf.select('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "177f9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def climate_geohash(lat, lon):\n",
    "    return geohash.encode(lat, lon, precision=3)\n",
    "\n",
    "def hotspot_geohash(lat, lon):\n",
    "    return geohash.encode(lat, lon, precision=5)\n",
    "\n",
    "def insertOne(result):\n",
    "    mongo_client = MongoClient(\n",
    "        host=f'{host_ip}',\n",
    "        port=27017\n",
    "    )\n",
    "    db = mongo_client['fit3182_assignment_db']\n",
    "    collection = db.a2_partB\n",
    "    collection.insert_one(result)\n",
    "    #print(\"INSERTED !\")\n",
    "    \n",
    "\n",
    "def func(batch_df,batch_id):\n",
    "    #retrieve all data from dataframe  (for kafka) into a python list for \n",
    "    entries = batch_df.collect()\n",
    "    #convert each entry into python dictionary\n",
    "    data = [entry.asDict() for entry in entries]\n",
    "    \n",
    "    \n",
    "    if data is not None:\n",
    "        #for each entry/row in one batch of 10 seconds\n",
    "        climate_report = None\n",
    "        climate_geohash3 = None\n",
    "        hotspot_hash = {}\n",
    "        #store the hotspot in precision 3\n",
    "        climate_hash = {}\n",
    "        \n",
    "        #each dict\n",
    "        for row in data: #select value column from kafka dict\n",
    "            byte_array = row['value']\n",
    "            # Decode the bytearray into a string\n",
    "            json_str = byte_array.decode('utf-8')\n",
    "\n",
    "            # Parse the JSON string into a dictionary\n",
    "            data_dict = json.loads(json_str)\n",
    "            \n",
    "            #climate report\n",
    "            if data_dict['producer'] == 'Producer_1':\n",
    "                #take only the first climate report\n",
    "                if climate_report is None:\n",
    "                    climate_report = data_dict\n",
    "                    \n",
    "                    \n",
    "                    climate_geohash3 = climate_geohash(climate_report['latitude'], climate_report['longitude'])\n",
    "                    #initialise a blank array in set to append hotspot later\n",
    "                    climate_hash[climate_geohash3] = []\n",
    "                    climate_report['hotspot'] = []\n",
    "\n",
    "                    \n",
    "            #hotspot report\n",
    "            else:\n",
    "                hotspot_dict = data_dict\n",
    "                #print(\"hotspot_dict: \", hotspot_dict)    #climate geohash as key only ke\n",
    "                \n",
    "                #hash the geohash among the hotspots\n",
    "                hotspot_geohash5 = hotspot_geohash(hotspot_dict['latitude'], hotspot_dict['longitude'])\n",
    "                \n",
    "                #maintain hotspot for not the same geohash string in precision 5\n",
    "                if hotspot_geohash5 not in hotspot_hash.keys():\n",
    "                    #accumulate the hotspot dictionary until cliamte report come\n",
    "                    hotspot_hash[hotspot_geohash5] = [hotspot_dict]\n",
    "                    \n",
    "                #add to the array with the same geohash key \n",
    "                else:\n",
    "                    hotspot_hash[hotspot_geohash5].append(hotspot_dict)\n",
    "                    \n",
    "                \n",
    "        #if climate report exist for each batch\n",
    "        if climate_report is not None:\n",
    "                #for each geohash5 hotspots do average of temp within 10 minutes\n",
    "            for key, value in hotspot_hash.items():\n",
    "                last_hp = value[-1]\n",
    "                #datetime string      #do again strptime to turn into datetime object\n",
    "                latest_min = datetime.strptime(last_hp['created_time'], '%H:%M:%S')\n",
    "                avg_temp = 0\n",
    "                avg_conf = 0\n",
    "                len_array = len(value)\n",
    "                #print(len_array)\n",
    "\n",
    "                #for each key average the surface temperature and confidence of the array in value\n",
    "                #by reducing array \n",
    "                for hp in value:\n",
    "                    # within 10 minutes from latest hotspot in each batch\n",
    "                    time_differnce = latest_min - datetime.strptime(hp['created_time'], '%H:%M:%S')\n",
    "                    if  time_differnce < timedelta(minutes = 10):\n",
    "                        #print(time_differnce)\n",
    "                        avg_temp += hp['surface_temperature_celcius']\n",
    "                        avg_conf += hp['confidence']\n",
    "\n",
    "                avg_temp = avg_temp/len_array\n",
    "                avg_conf = avg_conf/len_array\n",
    "                last_hp['surface_temperature_celcius'] = avg_temp\n",
    "                last_hp['avg_conf'] = avg_conf\n",
    "                #set it to only hotspot for this key\n",
    "                hotspot_hash[key] = last_hp\n",
    "\n",
    "\n",
    "                # Check if the first 3 characters of the key match the climate_geohash3\n",
    "                if str(key[:3]) == climate_geohash3:\n",
    "                    # If the key exists in climate_hash, append to it, otherwise create a new list\n",
    "                    if climate_geohash3 in climate_hash:\n",
    "                        climate_hash[climate_geohash3].append(hotspot_hash[key])\n",
    "                    else:\n",
    "                        climate_hash[climate_geohash3] = [hotspot_hash[key]]\n",
    "                    # Append the hotspots to the climate report\n",
    "                    #print(climate_hash[climate_geohash3])\n",
    "                    \n",
    "                    \n",
    "            climate_report['hotspot'] = climate_hash[climate_geohash3]\n",
    "                    #print(\"hotspots: \" + str(climate_report['hotspot']))\n",
    "            #print(len_array)\n",
    "            #CAUSE OF fire event\n",
    "            if climate_report['air_temperature_celcius'] > 20 and climate_report['GHI_w/m2'] > 180:\n",
    "                climate_report['fire_cause'] = 'natural'\n",
    "            else:\n",
    "                climate_report['fire_cause'] = 'other'\n",
    "        \n",
    "            print(climate_report)\n",
    "            insertOne(climate_report)        \n",
    "\n",
    "                \n",
    "db_writer = (\n",
    "    climatehotspot_sdf.writeStream\n",
    "    .outputMode('append')\n",
    "    .trigger(processingTime ='10 seconds') # one day \n",
    "    #apply a function to each aach of data with the internal of processing time\n",
    "    .foreachBatch(func) #batch and batch id and input\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d67c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'latitude': -36.7685, 'longitude': 142.7134, 'air_temperature_celcius': 14, 'relative_humidity': 48.2, 'windspeed_knots': 12.5, 'max_wind_speed': 19.0, 'precipitation': ' 0.03G', 'GHI_w/m2': 122, 'producer': 'Producer_1', 'date': '2024-01-02', 'hotspot': [], 'fire_cause': 'other'}\n",
      "{'latitude': -37.591, 'longitude': 149.33, 'air_temperature_celcius': 16, 'relative_humidity': 46.7, 'windspeed_knots': 10.0, 'max_wind_speed': 16.9, 'precipitation': ' 0.00I', 'GHI_w/m2': 141, 'producer': 'Producer_1', 'date': '2024-01-03', 'hotspot': [], 'fire_cause': 'other'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopping query.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = db_writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a31599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fd94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1ff827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814e35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b771b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab94be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab1aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d47212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad25f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash\n",
    "from pyspark.sql.functions import from_json, col, avg\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "\n",
    "# Define the schema for the Climate and Hotspot data payload\n",
    "# for from_json to know how to parse the structure\n",
    "\n",
    "#Schema to inform how parse the JSON object\n",
    "climate_schema = StructType([   #specify the data typem True to contain null values\n",
    "    StructField('latitude', StringType(), True),\n",
    "    StructField('longitude', DoubleType(), True),\n",
    "    StructField('air_temperature_celcius', IntegerType(), True),\n",
    "    StructField('relative_humidity', DoubleType(), True),\n",
    "    StructField('windspeed_knots', DoubleType(), True),\n",
    "    StructField('max_wind_speed', DoubleType(), True),\n",
    "    StructField('precipitation', StringType(), True),\n",
    "    StructField('GHI_w/m2', IntegerType(), True),\n",
    "    StructField('producer', StringType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('created_time', IntegerType(), True)\n",
    "])\n",
    "\n",
    "hotspot_schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"confidence\", DoubleType(), True),\n",
    "    StructField('surface_temperature_celcius', DoubleType(), True),\n",
    "    StructField(\"producer\", StringType(), True),\n",
    "    StructField(\"random_time\", IntegerType(), True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4917e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- air_temperature_celcius: integer (nullable = true)\n",
      " |-- relative_humidity: double (nullable = true)\n",
      " |-- windspeed_knots: double (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- precipitation: string (nullable = true)\n",
      " |-- GHI_w/m2: integer (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- created_time: timestamp (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "# Define the geohash UDF for climate\n",
    "def climate_geohash(lat, lon):\n",
    "    return geohash.encode(lat, lon, precision=3)\n",
    "\n",
    "# # Register UDFs in Spark\n",
    "climate_geohash_udf = udf(climate_geohash, StringType())\n",
    "spark.udf.register(\"climate_geohash\", climate_geohash_udf)\n",
    "\n",
    "\n",
    "# climate_sdf = (\n",
    "#     spark.readStream\n",
    "#     .format('kafka')\n",
    "#     .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "#     .option('subscribe', topic)\n",
    "#     .load() #value store the actual dataframe\n",
    "#     .select('value')\n",
    "#      #do processing after .load() to get into the dataframe and deal with row\n",
    "# #     .withColumn('geohash', climate_geohash_udf(col('latitude'), col('longtitude')))\n",
    "# #     .limit(1)  # Retains only the first row\n",
    "    \n",
    "# )\n",
    "\n",
    "# Deserialize the JSON payload from the 'value' column\n",
    "climate_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topic)\n",
    "    .load()  #load the JSON object to be in Dataframe, use climate_sdf.printSchema()  to see it\n",
    "    #deserialize the binary ‘value’ column into a format that allows you to access the ‘latitude’ and ‘longitude’ fields\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), climate_schema).alias(\"data\"))\n",
    "    #from_json: parse a JSON string and convert into a DataDrame of complex type StructType or MapType\n",
    "    #col(\"value\").cast(\"string\"): akes the ‘value’ column, which is in binary format, and \n",
    "    #casts it to a string type. This is necessary because from_json expects a JSON string as input.\n",
    "    #climate_schema: schema that defined which from_json will use to parse the JSON string. \n",
    "    #should match the structure of the JSON data you’re working with.\n",
    "    #.alias(\"data\"): This renames the resulting column from the from_json function to ‘data’.\n",
    "    .select(\"data.*\")  \n",
    "    #After JSON string parsed into a structured format\n",
    "    #select statement used to select all fields from the 'data' column since renamed\n",
    "    #.select(\"data.*\") select \n",
    "    .withColumn('geohash', climate_geohash_udf(col('latitude'), col('longitude')))\n",
    "    .withColumn('created_time', to_timestamp(col('created_time')))\n",
    "    .limit(1)  # Retains only the first row\n",
    ")\n",
    "\n",
    "#print the schema\n",
    "climate_sdf.printSchema() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "27107b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- surface_temperature_celcius: double (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- random_time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#all producers send to the same topic to make Kafka to join the stream\n",
    "hotspot_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topic_hotspot)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "hotspot_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topic)\n",
    "    .load()  \n",
    "    .select(from_json(col(\"value\").cast(\"string\"), hotspot_schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "hotspot_sdf.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "832aa4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geohash: string (nullable = true)\n",
      " |-- surface_temperature_celcius: double (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- air_temperature_celcius: integer (nullable = true)\n",
      " |-- relative_humidity: double (nullable = true)\n",
      " |-- windspeed_knots: double (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- precipitation: string (nullable = true)\n",
      " |-- GHI_w/m2: integer (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- created_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first\n",
    "# Define the geohash UDF for hotspot\n",
    "def hotspot_geohash(lat, lon):\n",
    "    return geohash.encode(lat, lon, precision=5)\n",
    "\n",
    "# Register the UDF for pyspark to group\n",
    "hotspot_geohash_udf = udf(hotspot_geohash, StringType())\n",
    "spark.udf.register(\"hotspot_geohash\", hotspot_geohash_udf)\n",
    "\n",
    "\n",
    "\n",
    "# climate_stream = climate_stream.withColumn('geohash', climate_geohash_udf(col('latitude'), col('longtitude')))\n",
    "hotspots_merge_sdf = (\n",
    "    hotspot_sdf\n",
    "    .withColumn('geohash', hotspot_geohash_udf(col('latitude'), col('longitude')))\n",
    "    #drop the rows with the same geohash at this time window\n",
    "    .dropDuplicates(['geohash'])\n",
    "    #laod data from source in dataframe format\n",
    ")\n",
    "\n",
    "# Update the result after dropDuplicates to update geohash to new value\n",
    "hotspots_merge_sdf = (\n",
    "    hotspot_sdf\n",
    "    .withColumn('geohash', hotspot_geohash_udf(col('latitude'), col('longitude')))\n",
    "    .dropDuplicates(['geohash'])\n",
    "    # Load data from source in DataFrame format\n",
    "    # Assuming load() is a method that loads the DataFrame, replace it with the actual method if different\n",
    "    # after dropDuplicates base on geohash, replace with lower precision geohash value\n",
    "    # for climat\n",
    "    .withColumn('geohash', climate_geohash_udf(col('latitude'), col('longitude')))\n",
    "    \n",
    ")\n",
    "\n",
    "# Alias the DataFrames before joining\n",
    "climate_sdf_alias = climate_sdf.alias(\"climate\")\n",
    "hotspots_merge_sdf_alias = hotspots_merge_sdf.alias(\"hotspot\")\n",
    "\n",
    "# Perform the join using the aliased DataFrames\n",
    "joined_df = climate_sdf_alias.join(hotspots_merge_sdf_alias, [\"geohash\"], how='left_outer')\n",
    "\n",
    "# Select columns using the alias  #select all columns \n",
    "joined_df = joined_df.select(\"climate.*\", \"hotspot.*\")\n",
    "\n",
    "# Apply a watermark to the joined DataFrame using the correct timestamp column\n",
    "# Replace 'created_time' with the actual timestamp column name from your data\n",
    "joined_df = joined_df.withWatermark(\"climate.created_time\", \"10 seconds\")\n",
    "\n",
    "# Perform aggregation\n",
    "averaged_df = joined_df.groupBy('climate.geohash').agg(\n",
    "    avg(col('hotspot.surface_temperature_celcius')).alias('surface_temperature_celcius'),\n",
    "    avg(col('hotspot.confidence')).alias('confidence'),\n",
    "    first(col('climate.air_temperature_celcius')).alias('air_temperature_celcius'),\n",
    "    first(col('climate.relative_humidity')).alias('relative_humidity'),\n",
    "    first(col('climate.windspeed_knots')).alias('windspeed_knots'),\n",
    "    first(col('climate.max_wind_speed')).alias('max_wind_speed'),\n",
    "    first(col('climate.precipitation')).alias('precipitation'),\n",
    "    first(col('climate.GHI_w/m2')).alias('GHI_w/m2'),\n",
    "    first(col('hotspot.producer')).alias('producer'),\n",
    "    first(col('climate.date')).alias('date'),\n",
    "    first(col('climate.latitude')).alias('latitude'),\n",
    "    first(col('climate.longitude')).alias('longitude'),\n",
    "    first(col('climate.created_time')).alias('created_time')\n",
    ")\n",
    "\n",
    "averaged_df.printSchema()\n",
    "# ## Each stream is a DStream of (key, value) pairs where 'key' could be a location \n",
    "# # Only include rows that have matching geohashes in both streams\n",
    "# # inner join since it is important that it has fire\n",
    "\n",
    "# #must join since from two different streams\n",
    "# joined_df = climate_sdf.join(hotspots_merge_sdf, on=['geohash'], how='left_outer') \n",
    "\n",
    "# # Select the disambiguated 'created_time' column from one of the DataFrames\n",
    "# # Assuming 'created_time' comes from the 'climate' DataFrame\n",
    "# joined_df = joined_df.select(\"climate_sdf.*\", \"hotspots_merge_sdf.geohash\")\n",
    "# # Apply a watermark to the joined DataFrame\n",
    "# joined_df = joined_df.withWatermark(\"created_time\", \"10 seconds\")\n",
    "\n",
    "# # Perform aggregation\n",
    "# averaged_df = joined_df.groupBy('geohash').agg(avg(col('surface_temperature_celcius')),avg(col('confidence')))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f04f0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging, print on console\n",
    "class DbWriter:\n",
    "    \n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.mongo_client = MongoClient(\n",
    "            host=f'{host_ip}',\n",
    "            port=27017\n",
    "        )\n",
    "        #use the same database, name: fit3182_db\n",
    "        self.db = self.mongo_client['fit3182_db']\n",
    "        return True\n",
    "    \n",
    "    #what ever row it receive at the time just process\n",
    "    def process(self, row):\n",
    "                #road of data from JSON string to \n",
    "        data = json.loads(row.value)\n",
    "        \n",
    "        record = {}\n",
    "        #climate\n",
    "        record['air_temperature_celcius'] = data.get('air_temperature_celcius')\n",
    "        record['relative_humidity'] = data.get('relative_humidity')\n",
    "        record['windspeed_knots'] = data.get('windspeed_knots')\n",
    "        record['max_wind_speed'] = data.get('max_wind_speed')\n",
    "        record['precipitation'] = data.get('precipitation')\n",
    "        record['GHI_w/m2'] = data.get('GHI_w/m2')\n",
    "        record['date'] = data.get('date')\n",
    "        record['latitude'] = data.get('latitude')\n",
    "        record['longtitude'] = data.get('longtitude')\n",
    "        \n",
    "        #hotspots\n",
    "        record['confidence'] = data.get('confidence')\n",
    "        record['surface_temperature_celcius'] = data.get('surface_temperature_celcius')\n",
    "        record['producer'] = data.get('producer')\n",
    "        print(record)\n",
    "        \n",
    "    def close(self, err):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f8efed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = (\n",
    "    # Initializes a streaming write for the climate_sdf DataFrame.\n",
    "    averaged_df.writeStream.format(\"console\")\n",
    "    # Output will be written to the standard console/output.\n",
    "    .option(\"checkpointLocation\", \"./hotspot_sdf_checkpoints\")\n",
    "    # Specifies the location for checkpointing, which allows streaming \n",
    "    # queries to be resilient to failures by storing the state.\n",
    "    .outputMode('append')  # Only new rows will be written to the output sink since the last trigger.\n",
    "    .trigger(processingTime = '10 seconds')\n",
    "    .foreach(DbWriter())  # Applies a foreach writer with DbWriter_climate instance.\n",
    "    # This indicates that for each row in the output, the DbWriter_climate class’s process method will be called.\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00034011",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py:1389\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [geohash#1475], [geohash#1475, avg(surface_temperature_celcius#3376) AS surface_temperature_celcius#3880, avg(confidence#3375) AS confidence#3882, first(air_temperature_celcius#1454, false) AS air_temperature_celcius#3884, first(relative_humidity#1455, false) AS relative_humidity#3886, first(windspeed_knots#1456, false) AS windspeed_knots#3888, first(max_wind_speed#1457, false) AS max_wind_speed#3890, first(precipitation#1458, false) AS precipitation#3892, first(GHI_w/m2#1459, false) AS GHI_w/m2#3894, first(producer#3377, false) AS producer#3896, first(date#1461, false) AS date#3898, first(latitude#1452, false) AS latitude#3900, first(longitude#1453, false) AS longitude#3902, first(created_time#1488-T10000ms, false) AS created_time#3904]\n+- EventTimeWatermark created_time#1488: timestamp, 10 seconds\n   +- Project [geohash#1475, latitude#1452, longitude#1453, air_temperature_celcius#1454, relative_humidity#1455, windspeed_knots#1456, max_wind_speed#1457, precipitation#1458, GHI_w/m2#1459, producer#1460, date#1461, created_time#1488, geohash#3815, latitude#3373, longitude#3374, confidence#3375, surface_temperature_celcius#3376, producer#3377, random_time#3378]\n      +- Project [geohash#3815, geohash#1475, latitude#1452, longitude#1453, air_temperature_celcius#1454, relative_humidity#1455, windspeed_knots#1456, max_wind_speed#1457, precipitation#1458, GHI_w/m2#1459, producer#1460, date#1461, created_time#1488, latitude#3373, longitude#3374, confidence#3375, surface_temperature_celcius#3376, producer#3377, random_time#3378]\n         +- Join LeftOuter, (geohash#1475 = geohash#3815)\n            :- SubqueryAlias climate\n            :  +- GlobalLimit 1\n            :     +- LocalLimit 1\n            :        +- Project [latitude#1452, longitude#1453, air_temperature_celcius#1454, relative_humidity#1455, windspeed_knots#1456, max_wind_speed#1457, precipitation#1458, GHI_w/m2#1459, producer#1460, date#1461, to_timestamp(created_time#1462, None, TimestampType, Some(Etc/UTC)) AS created_time#1488, geohash#1475]\n            :           +- Project [latitude#1452, longitude#1453, air_temperature_celcius#1454, relative_humidity#1455, windspeed_knots#1456, max_wind_speed#1457, precipitation#1458, GHI_w/m2#1459, producer#1460, date#1461, created_time#1462, climate_geohash(latitude#1452, longitude#1453)#1474 AS geohash#1475]\n            :              +- Project [data#1450.latitude AS latitude#1452, data#1450.longitude AS longitude#1453, data#1450.air_temperature_celcius AS air_temperature_celcius#1454, data#1450.relative_humidity AS relative_humidity#1455, data#1450.windspeed_knots AS windspeed_knots#1456, data#1450.max_wind_speed AS max_wind_speed#1457, data#1450.precipitation AS precipitation#1458, data#1450.GHI_w/m2 AS GHI_w/m2#1459, data#1450.producer AS producer#1460, data#1450.date AS date#1461, data#1450.created_time AS created_time#1462]\n            :                 +- Project [from_json(StructField(latitude,StringType,true), StructField(longitude,DoubleType,true), StructField(air_temperature_celcius,IntegerType,true), StructField(relative_humidity,DoubleType,true), StructField(windspeed_knots,DoubleType,true), StructField(max_wind_speed,DoubleType,true), StructField(precipitation,StringType,true), StructField(GHI_w/m2,IntegerType,true), StructField(producer,StringType,true), StructField(date,StringType,true), StructField(created_time,IntegerType,true), cast(value#1437 as string), Some(Etc/UTC)) AS data#1450]\n            :                    +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@3189691b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@63342902, [kafka.bootstrap.servers=172.16.55.43:9092, subscribe=A2_climate_topic], [key#1436, value#1437, topic#1438, partition#1439, offset#1440L, timestamp#1441, timestampType#1442], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4038c988,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 172.16.55.43:9092, subscribe -> A2_climate_topic),None), kafka, [key#1429, value#1430, topic#1431, partition#1432, offset#1433L, timestamp#1434, timestampType#1435]\n            +- SubqueryAlias hotspot\n               +- Project [latitude#3373, longitude#3374, confidence#3375, surface_temperature_celcius#3376, producer#3377, random_time#3378, climate_geohash(latitude#3373, longitude#3374)#3814 AS geohash#3815]\n                  +- Deduplicate [geohash#3806]\n                     +- Project [latitude#3373, longitude#3374, confidence#3375, surface_temperature_celcius#3376, producer#3377, random_time#3378, hotspot_geohash(latitude#3373, longitude#3374)#3805 AS geohash#3806]\n                        +- Project [data#3371.latitude AS latitude#3373, data#3371.longitude AS longitude#3374, data#3371.confidence AS confidence#3375, data#3371.surface_temperature_celcius AS surface_temperature_celcius#3376, data#3371.producer AS producer#3377, data#3371.random_time AS random_time#3378]\n                           +- Project [from_json(StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(confidence,DoubleType,true), StructField(surface_temperature_celcius,DoubleType,true), StructField(producer,StringType,true), StructField(random_time,IntegerType,true), cast(value#3358 as string), Some(Etc/UTC)) AS data#3371]\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@5f228c81, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@33256074, [kafka.bootstrap.servers=172.16.55.43:9092, subscribe=A2_climate_topic], [key#3357, value#3358, topic#3359, partition#3360, offset#3361L, timestamp#3362, timestampType#3363], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4038c988,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 172.16.55.43:9092, subscribe -> A2_climate_topic),None), kafka, [key#3350, value#3351, topic#3352, partition#3353, offset#3354L, timestamp#3355, timestampType#3356]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterrupted by CTRL-C. Stopping query.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mquery\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d580c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349810c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fd56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34318293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712a7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1883a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89c6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff2857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7a481f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbWriter_climate:\n",
    "    \"\"\"\n",
    "    Check whether it works in pyspark log\n",
    "    \"\"\"\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.mongo_client = MongoClient(\n",
    "            host=f'{host_ip}',\n",
    "            port=27017\n",
    "        )\n",
    "        #use the same database, name: fit3182_db\n",
    "        self.db = self.mongo_client['fit3182_db']\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    # called once per row of the result dataframe\n",
    "    # the current code DOES NOT handle duplicate processing\n",
    "    #   e.g., query fails and restarts just before current micro-batch was fully inserted\n",
    "    def process(self, row):\n",
    "        #passing JSON string from row.value\n",
    "        #into a dictionary data\n",
    "        data = json.loads(row.value)\n",
    "        \n",
    "        db_record = {}\n",
    "        db_record['air_temperature_celcius'] = data.get('air_temperature_celcius')\n",
    "        db_record['relative_humidity'] = data.get('relative_humidity')\n",
    "        db_record['windspeed_knots'] = data.get('windspeed_knots')\n",
    "        db_record['max_wind_speed'] = data.get('max_wind_speed')\n",
    "        db_record['precipitation'] = data.get('precipitation')\n",
    "        db_record['GHI_w/m2'] = data.get('GHI_w/m2')\n",
    "        db_record['producer'] = data.get('producer')\n",
    "        db_record['date'] = data.get('date')\n",
    "        #print(db_record)\n",
    "        #print heren not working\n",
    "                  \n",
    "        #update and insert\n",
    "        # New database has nothing so just insert\n",
    "        # later something with the same station then update\n",
    "        self.db['A2'].replace_one({'station': data.get('kerbsideid')}, db_record, upsert=True)\n",
    "    \n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08736cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = (\n",
    "    #initializes a streaming write for the parking_sdf DataFrame.\n",
    "    climate_sdf.writeStream.format(\"console\")\n",
    "    #output will be written to the standard console/output.\n",
    "    .option(\"checkpointLocation\", \"./climate_sdf_checkpoints\")\n",
    "    #Specifies the location for checkpointing, which allows streaming \n",
    "    #queries to be resilient to failures by storing the state.\n",
    "    #.outputMode('append'): Sets the output mode to ‘append’, meaning \n",
    "    #only new rows will be written to the output sink since the last trigger.\n",
    "    .outputMode('append').foreach(DbWriter_climate())\n",
    "    # Applies a foreach writer, which in this case is an instance of DbWriter(). \n",
    "    #This indicates that for each row in the output, the DbWriter() class’s process \n",
    "    #method will be called\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a14b396c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py:1389\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [geohash#1363], [geohash#1363, avg(surface_temperature_celcius#1422) AS avg(surface_temperature_celcius)#1568, avg(confidence#1421) AS avg(confidence)#1569]\n+- Project [geohash#1363, latitude#1342, longitude#1343, air_temperature_celcius#1344, relative_humidity#1345, windspeed_knots#1346, max_wind_speed#1347, precipitation#1348, GHI_w/m2#1349, producer#1350, date#1351, latitude#1419, longitude#1420, confidence#1421, surface_temperature_celcius#1422, created_time#1423, created_time#1424]\n   +- Join LeftOuter, (geohash#1363 = geohash#1524)\n      :- GlobalLimit 1\n      :  +- LocalLimit 1\n      :     +- Project [latitude#1342, longitude#1343, air_temperature_celcius#1344, relative_humidity#1345, windspeed_knots#1346, max_wind_speed#1347, precipitation#1348, GHI_w/m2#1349, producer#1350, date#1351, climate_geohash(latitude#1342, longitude#1343)#1362 AS geohash#1363]\n      :        +- Project [data#1340.latitude AS latitude#1342, data#1340.longitude AS longitude#1343, data#1340.air_temperature_celcius AS air_temperature_celcius#1344, data#1340.relative_humidity AS relative_humidity#1345, data#1340.windspeed_knots AS windspeed_knots#1346, data#1340.max_wind_speed AS max_wind_speed#1347, data#1340.precipitation AS precipitation#1348, data#1340.GHI_w/m2 AS GHI_w/m2#1349, data#1340.producer AS producer#1350, data#1340.date AS date#1351]\n      :           +- Project [from_json(StructField(latitude,StringType,true), StructField(longitude,DoubleType,true), StructField(air_temperature_celcius,IntegerType,true), StructField(relative_humidity,DoubleType,true), StructField(windspeed_knots,DoubleType,true), StructField(max_wind_speed,DoubleType,true), StructField(precipitation,StringType,true), StructField(GHI_w/m2,IntegerType,true), StructField(producer,StringType,true), StructField(date,StringType,true), cast(value#1327 as string), Some(Etc/UTC)) AS data#1340]\n      :              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6b840a63, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@485245e5, [kafka.bootstrap.servers=172.16.55.43:9092, subscribe=A2_climate_topic], [key#1326, value#1327, topic#1328, partition#1329, offset#1330L, timestamp#1331, timestampType#1332], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@30c9a569,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 172.16.55.43:9092, subscribe -> A2_climate_topic),None), kafka, [key#1319, value#1320, topic#1321, partition#1322, offset#1323L, timestamp#1324, timestampType#1325]\n      +- Project [latitude#1419, longitude#1420, confidence#1421, surface_temperature_celcius#1422, created_time#1423, created_time#1424, climate_geohash(latitude#1419, longitude#1420)#1523 AS geohash#1524]\n         +- Deduplicate [geohash#1515]\n            +- Project [latitude#1419, longitude#1420, confidence#1421, surface_temperature_celcius#1422, created_time#1423, created_time#1424, hotspot_geohash(latitude#1419, longitude#1420)#1514 AS geohash#1515]\n               +- Project [data#1417.latitude AS latitude#1419, data#1417.longitude AS longitude#1420, data#1417.confidence AS confidence#1421, data#1417.surface_temperature_celcius AS surface_temperature_celcius#1422, data#1417.created_time AS created_time#1423, data#1417.created_time AS created_time#1424]\n                  +- Project [from_json(StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(confidence,DoubleType,true), StructField(surface_temperature_celcius,DoubleType,true), StructField(created_time,StringType,true), StructField(created_time,IntegerType,true), cast(value#1404 as string), Some(Etc/UTC)) AS data#1417]\n                     +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@25aa5511, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@735e7fe0, [kafka.bootstrap.servers=172.16.55.43:9092, subscribe=A2_climate_topic], [key#1403, value#1404, topic#1405, partition#1406, offset#1407L, timestamp#1408, timestampType#1409], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@30c9a569,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 172.16.55.43:9092, subscribe -> A2_climate_topic),None), kafka, [key#1396, value#1397, topic#1398, partition#1399, offset#1400L, timestamp#1401, timestampType#1402]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterrupted by CTRL-C. Stopping query.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mquery\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6aaf9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import from_json, col\n",
    "# climate_stream = (\n",
    "#     spark.readStream\n",
    "#     .format('kafka')\n",
    "#     .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "#     .option('subscribe', topic)\n",
    "#     #load into dataframe for raw data from source(Kafka)\n",
    "#     .load()    #parse JSON string into structured format (Dataframe) based on schema\n",
    "#     .select(from_json(col('value').cast('string'), climate_schema).alias('data'))\n",
    "#     .select('data.*')         \n",
    "# )           #cast bytes in value column into string\n",
    "#             #alias rename value column into data\n",
    "#             #flatten by expandign fields of the data column into separate columns\n",
    "#             #select data column since renamed\n",
    "            \n",
    "# hotspot_AQUA_stream = (\n",
    "#     spark.readStream \\\n",
    "#     .format(\"kafka\")\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \n",
    "#     .option(\"subscribe\", topic_AQUA)\n",
    "#     .load()\n",
    "#     .select(from_json(col(\"value\").cast(\"string\"), hotspot_schema).alias(\"data\"))\n",
    "#     .select(\"data.*\")\n",
    "# )\n",
    "\n",
    "# hotspot_TERRA_stream = (\n",
    "#     spark.readStream \\\n",
    "#     .format(\"kafka\")\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \n",
    "#     .option(\"subscribe\", topic_TERRA)\n",
    "#     .load()\n",
    "#     .select(from_json(col(\"value\").cast(\"string\"), hotspot_schema).alias(\"data\"))\n",
    "#     .select(\"data.*\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43fe3d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longtitude: double (nullable = true)\n",
      " |-- air_temperature_celcius: integer (nullable = true)\n",
      " |-- relative_humidity: double (nullable = true)\n",
      " |-- windspeed_knots: double (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- precipitation: string (nullable = true)\n",
      " |-- GHI_w/m2: integer (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longtitude: double (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- surface_temperature: double (nullable = true)\n",
      " |-- created_time: string (nullable = true)\n",
      " |-- created_time: integer (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longtitude: double (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- surface_temperature: double (nullable = true)\n",
      " |-- created_time: string (nullable = true)\n",
      " |-- created_time: integer (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m hotspot_TERRA_stream\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Show the contents of the DataFrame\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mclimate_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m hotspot_AQUA_stream\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     53\u001b[0m hotspot_TERRA_stream\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka"
     ]
    }
   ],
   "source": [
    "# import geohash\n",
    "# from pyspark.sql.functions import col, udf\n",
    "# from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# # Define the geohash UDF for hotspot\n",
    "# def hotspot_geohash(lat, lon):\n",
    "#     return geohash.encode(lat, lon, precision=5)\n",
    "\n",
    "# # Register the UDF\n",
    "# hotspot_geohash_udf = udf(hotspot_geohash, StringType())\n",
    "# spark.udf.register(\"hotspot_geohash\", hotspot_geohash_udf)\n",
    "\n",
    "\n",
    "# # Define the geohash UDF for climate\n",
    "# def climate_geohash(lat, lon):\n",
    "#     return geohash.encode(lat, lon, precision=3)\n",
    "\n",
    "# # # Register UDFs in Spark\n",
    "# climate_geohash_udf = udf(climate_geohash, StringType())\n",
    "# spark.udf.register(\"climate_geohash\", climate_geohash_udf)\n",
    "\n",
    "\n",
    "\n",
    "# # Apply the geohash UDF to the DataFrame columns\n",
    "# climate_stream = climate_stream.withColumn('geohash', climate_geohash_udf(col('latitude'), col('longtitude')))\n",
    "# hotspot_AQUA_stream = hotspot_AQUA_stream.withColumn('geohash', hotspot_geohash_udf(col('latitude'), col('longtitude')))\n",
    "# hotspot_TERRA_stream = hotspot_TERRA_stream.withColumn('geohash', hotspot_geohash_udf(col('latitude'), col('longtitude')))\n",
    "\n",
    "\n",
    "# # Join the streams and drop duplicates\n",
    "# joined_hotspot_stream = hotspot_AQUA_stream.join(hotspot_TERRA_stream, 'geohash').dropDuplicates(['geohash'])\n",
    "# final_stream = climate_stream.join(joined_hotspot_stream, 'geohash')\n",
    "\n",
    "\n",
    "# # # Assuming you have a joined DataFrame called 'joined_df1_df2'\n",
    "# # for row in joined_hotspot_stream.collect():\n",
    "# #     geohash = row[\"geohash\"]\n",
    "# #     temperature = row[\"temperature\"]\n",
    "# #     hotspot_count = row[\"hotspot_count\"]\n",
    "# #     # Store relevant information in the dictionary\n",
    "# #     result_dict[geohash] = {\"temperature\": temperature, \"hotspot_count\": hotspot_count}\n",
    "# #     print(row)\n",
    "# # # Now 'result_dict' contains the processed data\n",
    "# # Print the schema of the DataFrame\n",
    "# climate_stream.printSchema()\n",
    "# hotspot_AQUA_stream.printSchema()\n",
    "# hotspot_TERRA_stream.printSchema()\n",
    "\n",
    "# # Show the contents of the DataFrame\n",
    "# climate_stream.show()\n",
    "# hotspot_AQUA_stream.show()\n",
    "# hotspot_TERRA_stream.show()\n",
    "\n",
    "# # After joining the streams\n",
    "# joined_hotspot_stream.printSchema()\n",
    "# final_stream.printSchema()\n",
    "\n",
    "# # Show the contents after joining\n",
    "# joined_hotspot_stream.show()\n",
    "# final_stream.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130215c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
