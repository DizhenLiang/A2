{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-geohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a70445b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "import geohash\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "#change ip address\n",
    "host_ip = \"172.16.33.120\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Streaming Application')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#append stream from multiple topic together\n",
    "topic = \"Producer1|Producer2|Producer3\"\n",
    "#topic_hotspot = 'A2_hotspot_topic'\n",
    "# topic_AQUA = 'Scenaries02'\n",
    "# topic_TERRA  = 'Scenaries03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e280884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribePattern', topic) #listen to multiple topics\n",
    "    .load()\n",
    ")\n",
    "kafka_sdf.printSchema()\n",
    "\n",
    "#select the value column in dataframe\n",
    "climatehotspot_sdf = kafka_sdf.select('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76991633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DbWriter:\n",
    "#     # called at the start of processing each partition in each output micro-batch\n",
    "#     def open(self, partition_id, epoch_id):\n",
    "#         self.mongo_client = MongoClient(\n",
    "#             host=f'{host_ip}',\n",
    "#             port=27017\n",
    "#         )\n",
    "#         self.db = self.mongo_client['fit3182_db']\n",
    "#         return True\n",
    "    \n",
    "#     # called once per row of the result dataframe\n",
    "#     # the current code DOES NOT handle duplicate processing\n",
    "#     #   e.g., query fails and restarts just before current micro-batch was fully inserted\n",
    "#     # deal fwith each row in datafram\n",
    "#     def process(self, row):\n",
    "#         data = json.loads(row.value)#for each row get value from (key, value) in value column\n",
    "#         print(data)\n",
    "#         db_record = {}\n",
    "#         db_record['_id'] = data.get('kerbsideid')\n",
    "#         db_record['latitude'] = data.get('location')[0]\n",
    "#         db_record['longitude'] = data.get('location')[1]\n",
    "#         db_record['status'] = data.get('status_description')\n",
    "#                     #update and insert\n",
    "#         self.db[topic].replace_one({'_id': data.get('kerbsideid')}, db_record, upsert=True)\n",
    "    \n",
    "#     # called once all rows have been processed (possibly with error)\n",
    "#     def close(self, err):\n",
    "#         self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177f9039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def climate_geohash(lat, lon):\n",
    "    return geohash.encode(lat, lon, precision=3)\n",
    "\n",
    "def hotspot_geohash(lat, lon):\n",
    "    return geohash.encode(lat, lon, precision=5)\n",
    "\n",
    "\n",
    "def func(batch_df,batch_id):\n",
    "    entries = batch_df.collect()\n",
    "    data = [entry.asDict() for entry in entries]\n",
    "    \n",
    "    \n",
    "    if data is not None:\n",
    "        #for each entry/row in one batch of 10 seconds\n",
    "        climate_report = None\n",
    "        climate_geohash3 = None\n",
    "        hotspot_hash = {}\n",
    "        #store the hotspot in precision 3\n",
    "        climate_hash = {}\n",
    "        \n",
    "        \n",
    "        for row in data:\n",
    "            byte_array = row['value']\n",
    "            # Decode the bytearray into a string\n",
    "            json_str = byte_array.decode('utf-8')\n",
    "\n",
    "            # Parse the JSON string into a dictionary\n",
    "            data_dict = json.loads(json_str)\n",
    "            \n",
    "            #climate report\n",
    "            if data_dict['producer'] == 'Producer_1':\n",
    "                #take only the first climate report\n",
    "                if climate_report is None:\n",
    "                    climate_report = data_dict\n",
    "                    \n",
    "                    \n",
    "                    climate_geohash3 = climate_geohash(climate_report['latitude'], climate_report['longitude'])\n",
    "                    #initialise a blank array in set to append hotspot later\n",
    "                    climate_hash[climate_geohash3] = []\n",
    "                    climate_report['hotspot'] = []\n",
    "                \n",
    "                else:\n",
    "                    #add the cumulated hotspot to the climate since added the clear climate_hash \n",
    "                    if climate_hash is not None:\n",
    "                        climate_report['hotspot'] += climate_hash[climate_geohash3]\n",
    "                        climate_hash = {}\n",
    "                        \n",
    "                \n",
    "                \n",
    "            #hotspot report\n",
    "            else:\n",
    "                hotspot_dict = data_dict\n",
    "                #print(\"hotspot_dict: \", hotspot_dict)    #climate geohash as key only ke\n",
    "                \n",
    "                #hash the geohash among the hotspots\n",
    "                hotspot_geohash5 = hotspot_geohash(hotspot_dict['latitude'], hotspot_dict['longitude'])\n",
    "                \n",
    "                #drop hotspots have the same geohash string in precision 5\n",
    "                if hotspot_geohash5 not in hotspot_hash.keys():\n",
    "                    #accumulate the hotspot dictionary until cliamte report come\n",
    "                    hotspot_hash[hotspot_geohash5] = hotspot_dict\n",
    "                \n",
    "                #if climate report exist\n",
    "                if climate_report is not None:\n",
    "                    \n",
    "                    for key, value in hotspot_hash.items():\n",
    "                        print(\"climate_geohash3: \" + str(climate_geohash3))\n",
    "                        print(\"hotspot_geohash: \" + str(key))\n",
    "                        # Check if the first 3 characters of the key match the climate_geohash3\n",
    "                        if str(key[:3]) == climate_geohash3:\n",
    "                            # If the key exists in climate_hash, append to it, otherwise create a new list\n",
    "                            if climate_geohash3 in climate_hash:\n",
    "                                climate_hash[climate_geohash3].append(hotspot_hash[key])\n",
    "                            else:\n",
    "                                climate_hash[climate_geohash3] = [hotspot_hash[key]]\n",
    "                            # Append the hotspots to the climate report\n",
    "                            print(climate_hash[climate_geohash3])\n",
    "                            climate_report['hotspot'].extend(climate_hash[climate_geohash3])\n",
    "                            print(\"hotspots: \" + str(climate_report['hotspot']))\n",
    "                    \n",
    "                    # After the loop, you can reset the hashes if needed\n",
    "                    # Note: This will remove all entries, so make sure this is intended\n",
    "                    climate_hash = {}\n",
    "                    hotspot_hash = {}\n",
    "                        \n",
    "        #add hotspot array to climate report\n",
    "        print(\"climate_report: \" + str(climate_report))\n",
    "#         if climate_report is not None:\n",
    "#             climate_report['hotspot'] = climate_hash[climate_geohash3]\n",
    "        \n",
    "            \n",
    "                        \n",
    "                   \n",
    "                \n",
    "    #self.db[topic].replace_one({'_id': data.get('kerbsideid')}, db_record, upsert=True)\n",
    "    # Merge Logic \n",
    "    \n",
    "\n",
    "db_writer = (\n",
    "    climatehotspot_sdf.writeStream\n",
    "    .outputMode('append')\n",
    "    .trigger(processingTime ='10 seconds') # one day \n",
    "    .foreachBatch(func)\n",
    ")\n",
    "\n",
    "\n",
    "# db_writer = (\n",
    "#     climatehotspot_sdf.writeStream\n",
    "#     .outputMode('append')\n",
    "#     .trigger(processingTime ='10 seconds') # one day \n",
    "#     .foreach(DbWriter())\n",
    "# )\n",
    "\n",
    "# writer = (\n",
    "#     climatehotspot_sdf.writeStream.format(\"console\")\n",
    "#     .option(\"checkpointLocation\", \"./parking_sdf_checkpoints\")\n",
    "#     .outputMode('complete').foreach(DbWriter())\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate_report: None\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1mjg\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r386c\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1nr7\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1m2u\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1kyu\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1jrd\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1s33\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1qmb\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1nx1\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1tcd\n",
      "climate_report: {'latitude': -37.644, 'longitude': 149.233, 'air_temperature_celcius': 22, 'relative_humidity': 58.0, 'windspeed_knots': 6.9, 'max_wind_speed': 12.0, 'precipitation': ' 0.00I', 'GHI_w/m2': 176, 'producer': 'Producer_1', 'date': '2024-01-14', 'created_time': 10, 'hotspot': []}\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1mtm\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1kt1\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1s2c\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1q7e\n",
      "[{'latitude': -37.3526, 'longitude': 143.9342, 'confidence': 75.0, 'surface_temperature_celcius': 49.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "hotspots: [{'latitude': -37.3526, 'longitude': 143.9342, 'confidence': 75.0, 'surface_temperature_celcius': 49.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1qpq\n",
      "[{'latitude': -36.6685, 'longitude': 143.7412, 'confidence': 79.0, 'surface_temperature_celcius': 52.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "hotspots: [{'latitude': -37.3526, 'longitude': 143.9342, 'confidence': 75.0, 'surface_temperature_celcius': 49.0, 'producer': 'Producer_3', 'random_time': 2}, {'latitude': -36.6685, 'longitude': 143.7412, 'confidence': 79.0, 'surface_temperature_celcius': 52.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1kcj\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1m6p\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1mdx\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1kxp\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1npg\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r32ym\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r30pe\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1t3d\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1mdj\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1mby\n",
      "climate_report: {'latitude': -37.288, 'longitude': 144.39, 'air_temperature_celcius': 12, 'relative_humidity': 49.9, 'windspeed_knots': 6.1, 'max_wind_speed': 13.0, 'precipitation': ' 0.02G', 'GHI_w/m2': 103, 'producer': 'Producer_1', 'date': '2024-01-15', 'created_time': 10, 'hotspot': [{'latitude': -37.3526, 'longitude': 143.9342, 'confidence': 75.0, 'surface_temperature_celcius': 49.0, 'producer': 'Producer_3', 'random_time': 2}, {'latitude': -36.6685, 'longitude': 143.7412, 'confidence': 79.0, 'surface_temperature_celcius': 52.0, 'producer': 'Producer_2', 'random_time': 1}]}\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1q5f\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1mfb\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1uu3\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1s2u\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1qr3\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r3362\n",
      "[{'latitude': -37.57, 'longitude': 148.034, 'confidence': 86.0, 'surface_temperature_celcius': 56.0, 'producer': 'Producer_3', 'random_time': 1}]\n",
      "hotspots: [{'latitude': -37.57, 'longitude': 148.034, 'confidence': 86.0, 'surface_temperature_celcius': 56.0, 'producer': 'Producer_3', 'random_time': 1}]\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1qsj\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1tb8\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r328s\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1s3g\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1w4w\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1kzp\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1mf9\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1mdv\n",
      "climate_report: {'latitude': -37.478, 'longitude': 148.117, 'air_temperature_celcius': 11, 'relative_humidity': 43.9, 'windspeed_knots': 11.2, 'max_wind_speed': 16.9, 'precipitation': ' 0.12G', 'GHI_w/m2': 99, 'producer': 'Producer_1', 'date': '2024-01-16', 'created_time': 10, 'hotspot': [{'latitude': -37.57, 'longitude': 148.034, 'confidence': 86.0, 'surface_temperature_celcius': 56.0, 'producer': 'Producer_3', 'random_time': 1}]}\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1qd0\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1w5t\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1wbw\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1q4e\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1rr9\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1ms3\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1rr0\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1npq\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1mg4\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1qsj\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1kmz\n",
      "[{'latitude': -36.9142, 'longitude': 141.3013, 'confidence': 57.0, 'surface_temperature_celcius': 40.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "hotspots: [{'latitude': -36.9142, 'longitude': 141.3013, 'confidence': 57.0, 'surface_temperature_celcius': 40.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1wdm\n",
      "climate_geohash3: r1k\n",
      "hotspot_geohash: r1uub\n",
      "climate_report: {'latitude': -37.588, 'longitude': 141.259, 'air_temperature_celcius': 14, 'relative_humidity': 52.8, 'windspeed_knots': 9.3, 'max_wind_speed': 18.1, 'precipitation': ' 0.98G', 'GHI_w/m2': 117, 'producer': 'Producer_1', 'date': '2024-01-17', 'created_time': 10, 'hotspot': [{'latitude': -36.9142, 'longitude': 141.3013, 'confidence': 57.0, 'surface_temperature_celcius': 40.0, 'producer': 'Producer_2', 'random_time': 1}]}\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1sbb\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1wbh\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1mde\n",
      "[{'latitude': -37.5025, 'longitude': 142.9011, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "hotspots: [{'latitude': -37.5025, 'longitude': 142.9011, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1rm5\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1rcw\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1mvb\n",
      "[{'latitude': -36.9205, 'longitude': 143.0889, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "hotspots: [{'latitude': -37.5025, 'longitude': 142.9011, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -36.9205, 'longitude': 143.0889, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1wbv\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1wcu\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1mr0\n",
      "[{'latitude': -36.7149, 'longitude': 142.3934, 'confidence': 65.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}]\n",
      "hotspots: [{'latitude': -37.5025, 'longitude': 142.9011, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -36.9205, 'longitude': 143.0889, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -36.7149, 'longitude': 142.3934, 'confidence': 65.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}]\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1w2s\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1mr6\n",
      "[{'latitude': -36.6507, 'longitude': 142.4998, 'confidence': 86.0, 'surface_temperature_celcius': 61.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "hotspots: [{'latitude': -37.5025, 'longitude': 142.9011, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -36.9205, 'longitude': 143.0889, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -36.7149, 'longitude': 142.3934, 'confidence': 65.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}, {'latitude': -36.6507, 'longitude': 142.4998, 'confidence': 86.0, 'surface_temperature_celcius': 61.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1qn2\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1k9q\n",
      "climate_report: {'latitude': -37.479, 'longitude': 143.358, 'air_temperature_celcius': 23, 'relative_humidity': 60.6, 'windspeed_knots': 10.1, 'max_wind_speed': 26.0, 'precipitation': ' 0.00I', 'GHI_w/m2': 180, 'producer': 'Producer_1', 'date': '2024-01-18', 'created_time': 10, 'hotspot': [{'latitude': -37.5025, 'longitude': 142.9011, 'confidence': 73.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -36.9205, 'longitude': 143.0889, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -36.7149, 'longitude': 142.3934, 'confidence': 65.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}, {'latitude': -36.6507, 'longitude': 142.4998, 'confidence': 86.0, 'surface_temperature_celcius': 61.0, 'producer': 'Producer_3', 'random_time': 2}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate_report: {'latitude': -36.418, 'longitude': 141.596, 'air_temperature_celcius': 14, 'relative_humidity': 50.1, 'windspeed_knots': 8.7, 'max_wind_speed': 15.0, 'precipitation': ' 0.00G', 'GHI_w/m2': 120, 'producer': 'Producer_1', 'date': '2024-01-19', 'created_time': 10, 'hotspot': []}\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r30qt\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1mbb\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1m2s\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1men\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1q5g\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1m48\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1w65\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1mp9\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1mjb\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1mqr\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r1mfy\n",
      "climate_geohash3: r36\n",
      "hotspot_geohash: r3379\n",
      "climate_report: {'latitude': -37.609, 'longitude': 149.32, 'air_temperature_celcius': 16, 'relative_humidity': 48.3, 'windspeed_knots': 9.4, 'max_wind_speed': 14.0, 'precipitation': ' 0.01G', 'GHI_w/m2': 139, 'producer': 'Producer_1', 'date': '2024-01-20', 'created_time': 10, 'hotspot': []}\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1mrv\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1npq\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1m37\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1s34\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1w5j\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1mf9\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1mbx\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1mnd\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1mtv\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1kwv\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1mdr\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1rrz\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1rpf\n",
      "climate_geohash3: r30\n",
      "hotspot_geohash: r1mqn\n",
      "climate_report: {'latitude': -38.498, 'longitude': 146.95, 'air_temperature_celcius': 5, 'relative_humidity': 38.6, 'windspeed_knots': 1.8, 'max_wind_speed': 5.1, 'precipitation': ' 0.00I', 'GHI_w/m2': 47, 'producer': 'Producer_1', 'date': '2024-01-21', 'created_time': 10, 'hotspot': []}\n",
      "climate_report: {'latitude': -37.1815, 'longitude': 146.7777, 'air_temperature_celcius': 17, 'relative_humidity': 50.4, 'windspeed_knots': 10.8, 'max_wind_speed': 16.9, 'precipitation': ' 0.00I', 'GHI_w/m2': 145, 'producer': 'Producer_1', 'date': '2024-01-22', 'created_time': 10, 'hotspot': []}\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1m86\n",
      "[{'latitude': -37.9049, 'longitude': 142.8528, 'confidence': 59.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}]\n",
      "hotspots: [{'latitude': -37.9049, 'longitude': 142.8528, 'confidence': 59.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}]\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1mpe\n",
      "[{'latitude': -36.6146, 'longitude': 142.2013, 'confidence': 77.0, 'surface_temperature_celcius': 51.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "hotspots: [{'latitude': -37.9049, 'longitude': 142.8528, 'confidence': 59.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}, {'latitude': -36.6146, 'longitude': 142.2013, 'confidence': 77.0, 'surface_temperature_celcius': 51.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1ms9\n",
      "[{'latitude': -37.1603, 'longitude': 142.8088, 'confidence': 65.0, 'surface_temperature_celcius': 50.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "hotspots: [{'latitude': -37.9049, 'longitude': 142.8528, 'confidence': 59.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}, {'latitude': -36.6146, 'longitude': 142.2013, 'confidence': 77.0, 'surface_temperature_celcius': 51.0, 'producer': 'Producer_3', 'random_time': 2}, {'latitude': -37.1603, 'longitude': 142.8088, 'confidence': 65.0, 'surface_temperature_celcius': 50.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r32rq\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1w5t\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1rr3\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r3373\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1w9j\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1kyt\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1m25\n",
      "[{'latitude': -37.9284, 'longitude': 142.5465, 'confidence': 78.0, 'surface_temperature_celcius': 51.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "hotspots: [{'latitude': -37.9049, 'longitude': 142.8528, 'confidence': 59.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}, {'latitude': -36.6146, 'longitude': 142.2013, 'confidence': 77.0, 'surface_temperature_celcius': 51.0, 'producer': 'Producer_3', 'random_time': 2}, {'latitude': -37.1603, 'longitude': 142.8088, 'confidence': 65.0, 'surface_temperature_celcius': 50.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -37.9284, 'longitude': 142.5465, 'confidence': 78.0, 'surface_temperature_celcius': 51.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1kyn\n",
      "climate_geohash3: r1m\n",
      "hotspot_geohash: r1q0q\n",
      "climate_report: {'latitude': -37.618, 'longitude': 143.0013, 'air_temperature_celcius': 14, 'relative_humidity': 36.0, 'windspeed_knots': 15.9, 'max_wind_speed': 22.0, 'precipitation': ' 0.00I', 'GHI_w/m2': 134, 'producer': 'Producer_1', 'date': '2024-01-23', 'created_time': 10, 'hotspot': [{'latitude': -37.9049, 'longitude': 142.8528, 'confidence': 59.0, 'surface_temperature_celcius': 47.0, 'producer': 'Producer_2', 'random_time': 2}, {'latitude': -36.6146, 'longitude': 142.2013, 'confidence': 77.0, 'surface_temperature_celcius': 51.0, 'producer': 'Producer_3', 'random_time': 2}, {'latitude': -37.1603, 'longitude': 142.8088, 'confidence': 65.0, 'surface_temperature_celcius': 50.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -37.9284, 'longitude': 142.5465, 'confidence': 78.0, 'surface_temperature_celcius': 51.0, 'producer': 'Producer_3', 'random_time': 2}]}\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1s8v\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1rnp\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1s9j\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1mmz\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1s97\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1s8s\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1q0g\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1vcf\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1kbc\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1m64\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1xf9\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1wc2\n",
      "climate_geohash3: r1n\n",
      "hotspot_geohash: r1wj4\n",
      "climate_report: {'latitude': -38.116, 'longitude': 143.818, 'air_temperature_celcius': 10, 'relative_humidity': 46.8, 'windspeed_knots': 5.3, 'max_wind_speed': 9.9, 'precipitation': ' 0.00I', 'GHI_w/m2': 88, 'producer': 'Producer_1', 'date': '2024-01-24', 'created_time': 10, 'hotspot': []}\n",
      "climate_report: {'latitude': -36.778, 'longitude': 145.1635, 'air_temperature_celcius': 17, 'relative_humidity': 46.3, 'windspeed_knots': 11.7, 'max_wind_speed': 20.0, 'precipitation': ' 0.02G', 'GHI_w/m2': 150, 'producer': 'Producer_1', 'date': '2024-01-25', 'created_time': 10, 'hotspot': []}\n",
      "climate_report: {'latitude': -37.062, 'longitude': 141.373, 'air_temperature_celcius': 22, 'relative_humidity': 46.8, 'windspeed_knots': 8.0, 'max_wind_speed': 16.9, 'precipitation': ' 0.00I', 'GHI_w/m2': 194, 'producer': 'Producer_1', 'date': '2024-01-26', 'created_time': 10, 'hotspot': []}\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1m2b\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r3362\n",
      "[{'latitude': -37.559, 'longitude': 148.037, 'confidence': 56.0, 'surface_temperature_celcius': 41.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "hotspots: [{'latitude': -37.559, 'longitude': 148.037, 'confidence': 56.0, 'surface_temperature_celcius': 41.0, 'producer': 'Producer_3', 'random_time': 2}]\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1m9d\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1mqt\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1w46\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1mbe\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r336g\n",
      "[{'latitude': -37.4754, 'longitude': 148.161, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "hotspots: [{'latitude': -37.559, 'longitude': 148.037, 'confidence': 56.0, 'surface_temperature_celcius': 41.0, 'producer': 'Producer_3', 'random_time': 2}, {'latitude': -37.4754, 'longitude': 148.161, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1kq4\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1mpy\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r336c\n",
      "[{'latitude': -37.485, 'longitude': 148.095, 'confidence': 71.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 2}]\n",
      "hotspots: [{'latitude': -37.559, 'longitude': 148.037, 'confidence': 56.0, 'surface_temperature_celcius': 41.0, 'producer': 'Producer_3', 'random_time': 2}, {'latitude': -37.4754, 'longitude': 148.161, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -37.485, 'longitude': 148.095, 'confidence': 71.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 2}]\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r30m9\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1kw7\n",
      "climate_geohash3: r33\n",
      "hotspot_geohash: r1mbb\n",
      "climate_report: {'latitude': -37.446, 'longitude': 148.102, 'air_temperature_celcius': 10, 'relative_humidity': 39.4, 'windspeed_knots': 9.6, 'max_wind_speed': 14.0, 'precipitation': ' 0.00G', 'GHI_w/m2': 93, 'producer': 'Producer_1', 'date': '2024-01-27', 'created_time': 10, 'hotspot': [{'latitude': -37.559, 'longitude': 148.037, 'confidence': 56.0, 'surface_temperature_celcius': 41.0, 'producer': 'Producer_3', 'random_time': 2}, {'latitude': -37.4754, 'longitude': 148.161, 'confidence': 67.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 1}, {'latitude': -37.485, 'longitude': 148.095, 'confidence': 71.0, 'surface_temperature_celcius': 43.0, 'producer': 'Producer_2', 'random_time': 2}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r3390\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1xc9\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1tks\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r384w\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1mf0\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1t0t\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1km1\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1m3m\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1npf\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1q47\n",
      "[{'latitude': -37.5436, 'longitude': 143.6122, 'confidence': 80.0, 'surface_temperature_celcius': 54.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "hotspots: [{'latitude': -37.5436, 'longitude': 143.6122, 'confidence': 80.0, 'surface_temperature_celcius': 54.0, 'producer': 'Producer_2', 'random_time': 1}]\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1m8e\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1w9p\n",
      "climate_geohash3: r1q\n",
      "hotspot_geohash: r1kr1\n",
      "climate_report: {'latitude': -36.9364, 'longitude': 143.4996, 'air_temperature_celcius': 14, 'relative_humidity': 41.7, 'windspeed_knots': 12.3, 'max_wind_speed': 18.1, 'precipitation': ' 0.00G', 'GHI_w/m2': 128, 'producer': 'Producer_1', 'date': '2024-01-28', 'created_time': 10, 'hotspot': [{'latitude': -37.5436, 'longitude': 143.6122, 'confidence': 80.0, 'surface_temperature_celcius': 54.0, 'producer': 'Producer_2', 'random_time': 1}]}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = db_writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a31599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab94be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab1aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d47212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad25f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash\n",
    "from pyspark.sql.functions import from_json, col, avg\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "\n",
    "# Define the schema for the Climate and Hotspot data payload\n",
    "# for from_json to know how to parse the structure\n",
    "\n",
    "#Schema to inform how parse the JSON object\n",
    "climate_schema = StructType([   #specify the data typem True to contain null values\n",
    "    StructField('latitude', StringType(), True),\n",
    "    StructField('longitude', DoubleType(), True),\n",
    "    StructField('air_temperature_celcius', IntegerType(), True),\n",
    "    StructField('relative_humidity', DoubleType(), True),\n",
    "    StructField('windspeed_knots', DoubleType(), True),\n",
    "    StructField('max_wind_speed', DoubleType(), True),\n",
    "    StructField('precipitation', StringType(), True),\n",
    "    StructField('GHI_w/m2', IntegerType(), True),\n",
    "    StructField('producer', StringType(), True),\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('created_time', IntegerType(), True)\n",
    "])\n",
    "\n",
    "hotspot_schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"confidence\", DoubleType(), True),\n",
    "    StructField('surface_temperature_celcius', DoubleType(), True),\n",
    "    StructField(\"producer\", StringType(), True),\n",
    "    StructField(\"random_time\", IntegerType(), True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4917e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- air_temperature_celcius: integer (nullable = true)\n",
      " |-- relative_humidity: double (nullable = true)\n",
      " |-- windspeed_knots: double (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- precipitation: string (nullable = true)\n",
      " |-- GHI_w/m2: integer (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- created_time: timestamp (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "# Define the geohash UDF for climate\n",
    "def climate_geohash(lat, lon):\n",
    "    return geohash.encode(lat, lon, precision=3)\n",
    "\n",
    "# # Register UDFs in Spark\n",
    "climate_geohash_udf = udf(climate_geohash, StringType())\n",
    "spark.udf.register(\"climate_geohash\", climate_geohash_udf)\n",
    "\n",
    "\n",
    "# climate_sdf = (\n",
    "#     spark.readStream\n",
    "#     .format('kafka')\n",
    "#     .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "#     .option('subscribe', topic)\n",
    "#     .load() #value store the actual dataframe\n",
    "#     .select('value')\n",
    "#      #do processing after .load() to get into the dataframe and deal with row\n",
    "# #     .withColumn('geohash', climate_geohash_udf(col('latitude'), col('longtitude')))\n",
    "# #     .limit(1)  # Retains only the first row\n",
    "    \n",
    "# )\n",
    "\n",
    "# Deserialize the JSON payload from the 'value' column\n",
    "climate_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topic)\n",
    "    .load()  #load the JSON object to be in Dataframe, use climate_sdf.printSchema()  to see it\n",
    "    #deserialize the binary ‘value’ column into a format that allows you to access the ‘latitude’ and ‘longitude’ fields\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), climate_schema).alias(\"data\"))\n",
    "    #from_json: parse a JSON string and convert into a DataDrame of complex type StructType or MapType\n",
    "    #col(\"value\").cast(\"string\"): akes the ‘value’ column, which is in binary format, and \n",
    "    #casts it to a string type. This is necessary because from_json expects a JSON string as input.\n",
    "    #climate_schema: schema that defined which from_json will use to parse the JSON string. \n",
    "    #should match the structure of the JSON data you’re working with.\n",
    "    #.alias(\"data\"): This renames the resulting column from the from_json function to ‘data’.\n",
    "    .select(\"data.*\")  \n",
    "    #After JSON string parsed into a structured format\n",
    "    #select statement used to select all fields from the 'data' column since renamed\n",
    "    #.select(\"data.*\") select \n",
    "    .withColumn('geohash', climate_geohash_udf(col('latitude'), col('longitude')))\n",
    "    .withColumn('created_time', to_timestamp(col('created_time')))\n",
    "    .limit(1)  # Retains only the first row\n",
    ")\n",
    "\n",
    "#print the schema\n",
    "climate_sdf.printSchema() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "27107b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- surface_temperature_celcius: double (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- random_time: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#all producers send to the same topic to make Kafka to join the stream\n",
    "hotspot_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topic_hotspot)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "hotspot_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', topic)\n",
    "    .load()  \n",
    "    .select(from_json(col(\"value\").cast(\"string\"), hotspot_schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "hotspot_sdf.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "832aa4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geohash: string (nullable = true)\n",
      " |-- surface_temperature_celcius: double (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- air_temperature_celcius: integer (nullable = true)\n",
      " |-- relative_humidity: double (nullable = true)\n",
      " |-- windspeed_knots: double (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- precipitation: string (nullable = true)\n",
      " |-- GHI_w/m2: integer (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- created_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first\n",
    "# Define the geohash UDF for hotspot\n",
    "def hotspot_geohash(lat, lon):\n",
    "    return geohash.encode(lat, lon, precision=5)\n",
    "\n",
    "# Register the UDF for pyspark to group\n",
    "hotspot_geohash_udf = udf(hotspot_geohash, StringType())\n",
    "spark.udf.register(\"hotspot_geohash\", hotspot_geohash_udf)\n",
    "\n",
    "\n",
    "\n",
    "# climate_stream = climate_stream.withColumn('geohash', climate_geohash_udf(col('latitude'), col('longtitude')))\n",
    "hotspots_merge_sdf = (\n",
    "    hotspot_sdf\n",
    "    .withColumn('geohash', hotspot_geohash_udf(col('latitude'), col('longitude')))\n",
    "    #drop the rows with the same geohash at this time window\n",
    "    .dropDuplicates(['geohash'])\n",
    "    #laod data from source in dataframe format\n",
    ")\n",
    "\n",
    "# Update the result after dropDuplicates to update geohash to new value\n",
    "hotspots_merge_sdf = (\n",
    "    hotspot_sdf\n",
    "    .withColumn('geohash', hotspot_geohash_udf(col('latitude'), col('longitude')))\n",
    "    .dropDuplicates(['geohash'])\n",
    "    # Load data from source in DataFrame format\n",
    "    # Assuming load() is a method that loads the DataFrame, replace it with the actual method if different\n",
    "    # after dropDuplicates base on geohash, replace with lower precision geohash value\n",
    "    # for climat\n",
    "    .withColumn('geohash', climate_geohash_udf(col('latitude'), col('longitude')))\n",
    "    \n",
    ")\n",
    "\n",
    "# Alias the DataFrames before joining\n",
    "climate_sdf_alias = climate_sdf.alias(\"climate\")\n",
    "hotspots_merge_sdf_alias = hotspots_merge_sdf.alias(\"hotspot\")\n",
    "\n",
    "# Perform the join using the aliased DataFrames\n",
    "joined_df = climate_sdf_alias.join(hotspots_merge_sdf_alias, [\"geohash\"], how='left_outer')\n",
    "\n",
    "# Select columns using the alias  #select all columns \n",
    "joined_df = joined_df.select(\"climate.*\", \"hotspot.*\")\n",
    "\n",
    "# Apply a watermark to the joined DataFrame using the correct timestamp column\n",
    "# Replace 'created_time' with the actual timestamp column name from your data\n",
    "joined_df = joined_df.withWatermark(\"climate.created_time\", \"10 seconds\")\n",
    "\n",
    "# Perform aggregation\n",
    "averaged_df = joined_df.groupBy('climate.geohash').agg(\n",
    "    avg(col('hotspot.surface_temperature_celcius')).alias('surface_temperature_celcius'),\n",
    "    avg(col('hotspot.confidence')).alias('confidence'),\n",
    "    first(col('climate.air_temperature_celcius')).alias('air_temperature_celcius'),\n",
    "    first(col('climate.relative_humidity')).alias('relative_humidity'),\n",
    "    first(col('climate.windspeed_knots')).alias('windspeed_knots'),\n",
    "    first(col('climate.max_wind_speed')).alias('max_wind_speed'),\n",
    "    first(col('climate.precipitation')).alias('precipitation'),\n",
    "    first(col('climate.GHI_w/m2')).alias('GHI_w/m2'),\n",
    "    first(col('hotspot.producer')).alias('producer'),\n",
    "    first(col('climate.date')).alias('date'),\n",
    "    first(col('climate.latitude')).alias('latitude'),\n",
    "    first(col('climate.longitude')).alias('longitude'),\n",
    "    first(col('climate.created_time')).alias('created_time')\n",
    ")\n",
    "\n",
    "averaged_df.printSchema()\n",
    "# ## Each stream is a DStream of (key, value) pairs where 'key' could be a location \n",
    "# # Only include rows that have matching geohashes in both streams\n",
    "# # inner join since it is important that it has fire\n",
    "\n",
    "# #must join since from two different streams\n",
    "# joined_df = climate_sdf.join(hotspots_merge_sdf, on=['geohash'], how='left_outer') \n",
    "\n",
    "# # Select the disambiguated 'created_time' column from one of the DataFrames\n",
    "# # Assuming 'created_time' comes from the 'climate' DataFrame\n",
    "# joined_df = joined_df.select(\"climate_sdf.*\", \"hotspots_merge_sdf.geohash\")\n",
    "# # Apply a watermark to the joined DataFrame\n",
    "# joined_df = joined_df.withWatermark(\"created_time\", \"10 seconds\")\n",
    "\n",
    "# # Perform aggregation\n",
    "# averaged_df = joined_df.groupBy('geohash').agg(avg(col('surface_temperature_celcius')),avg(col('confidence')))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f04f0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging, print on console\n",
    "class DbWriter:\n",
    "    \n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.mongo_client = MongoClient(\n",
    "            host=f'{host_ip}',\n",
    "            port=27017\n",
    "        )\n",
    "        #use the same database, name: fit3182_db\n",
    "        self.db = self.mongo_client['fit3182_db']\n",
    "        return True\n",
    "    \n",
    "    #what ever row it receive at the time just process\n",
    "    def process(self, row):\n",
    "                #road of data from JSON string to \n",
    "        data = json.loads(row.value)\n",
    "        \n",
    "        record = {}\n",
    "        #climate\n",
    "        record['air_temperature_celcius'] = data.get('air_temperature_celcius')\n",
    "        record['relative_humidity'] = data.get('relative_humidity')\n",
    "        record['windspeed_knots'] = data.get('windspeed_knots')\n",
    "        record['max_wind_speed'] = data.get('max_wind_speed')\n",
    "        record['precipitation'] = data.get('precipitation')\n",
    "        record['GHI_w/m2'] = data.get('GHI_w/m2')\n",
    "        record['date'] = data.get('date')\n",
    "        record['latitude'] = data.get('latitude')\n",
    "        record['longtitude'] = data.get('longtitude')\n",
    "        \n",
    "        #hotspots\n",
    "        record['confidence'] = data.get('confidence')\n",
    "        record['surface_temperature_celcius'] = data.get('surface_temperature_celcius')\n",
    "        record['producer'] = data.get('producer')\n",
    "        print(record)\n",
    "        \n",
    "    def close(self, err):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f8efed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = (\n",
    "    # Initializes a streaming write for the climate_sdf DataFrame.\n",
    "    averaged_df.writeStream.format(\"console\")\n",
    "    # Output will be written to the standard console/output.\n",
    "    .option(\"checkpointLocation\", \"./hotspot_sdf_checkpoints\")\n",
    "    # Specifies the location for checkpointing, which allows streaming \n",
    "    # queries to be resilient to failures by storing the state.\n",
    "    .outputMode('append')  # Only new rows will be written to the output sink since the last trigger.\n",
    "    .trigger(processingTime = '10 seconds')\n",
    "    .foreach(DbWriter())  # Applies a foreach writer with DbWriter_climate instance.\n",
    "    # This indicates that for each row in the output, the DbWriter_climate class’s process method will be called.\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00034011",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py:1389\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [geohash#1475], [geohash#1475, avg(surface_temperature_celcius#3376) AS surface_temperature_celcius#3880, avg(confidence#3375) AS confidence#3882, first(air_temperature_celcius#1454, false) AS air_temperature_celcius#3884, first(relative_humidity#1455, false) AS relative_humidity#3886, first(windspeed_knots#1456, false) AS windspeed_knots#3888, first(max_wind_speed#1457, false) AS max_wind_speed#3890, first(precipitation#1458, false) AS precipitation#3892, first(GHI_w/m2#1459, false) AS GHI_w/m2#3894, first(producer#3377, false) AS producer#3896, first(date#1461, false) AS date#3898, first(latitude#1452, false) AS latitude#3900, first(longitude#1453, false) AS longitude#3902, first(created_time#1488-T10000ms, false) AS created_time#3904]\n+- EventTimeWatermark created_time#1488: timestamp, 10 seconds\n   +- Project [geohash#1475, latitude#1452, longitude#1453, air_temperature_celcius#1454, relative_humidity#1455, windspeed_knots#1456, max_wind_speed#1457, precipitation#1458, GHI_w/m2#1459, producer#1460, date#1461, created_time#1488, geohash#3815, latitude#3373, longitude#3374, confidence#3375, surface_temperature_celcius#3376, producer#3377, random_time#3378]\n      +- Project [geohash#3815, geohash#1475, latitude#1452, longitude#1453, air_temperature_celcius#1454, relative_humidity#1455, windspeed_knots#1456, max_wind_speed#1457, precipitation#1458, GHI_w/m2#1459, producer#1460, date#1461, created_time#1488, latitude#3373, longitude#3374, confidence#3375, surface_temperature_celcius#3376, producer#3377, random_time#3378]\n         +- Join LeftOuter, (geohash#1475 = geohash#3815)\n            :- SubqueryAlias climate\n            :  +- GlobalLimit 1\n            :     +- LocalLimit 1\n            :        +- Project [latitude#1452, longitude#1453, air_temperature_celcius#1454, relative_humidity#1455, windspeed_knots#1456, max_wind_speed#1457, precipitation#1458, GHI_w/m2#1459, producer#1460, date#1461, to_timestamp(created_time#1462, None, TimestampType, Some(Etc/UTC)) AS created_time#1488, geohash#1475]\n            :           +- Project [latitude#1452, longitude#1453, air_temperature_celcius#1454, relative_humidity#1455, windspeed_knots#1456, max_wind_speed#1457, precipitation#1458, GHI_w/m2#1459, producer#1460, date#1461, created_time#1462, climate_geohash(latitude#1452, longitude#1453)#1474 AS geohash#1475]\n            :              +- Project [data#1450.latitude AS latitude#1452, data#1450.longitude AS longitude#1453, data#1450.air_temperature_celcius AS air_temperature_celcius#1454, data#1450.relative_humidity AS relative_humidity#1455, data#1450.windspeed_knots AS windspeed_knots#1456, data#1450.max_wind_speed AS max_wind_speed#1457, data#1450.precipitation AS precipitation#1458, data#1450.GHI_w/m2 AS GHI_w/m2#1459, data#1450.producer AS producer#1460, data#1450.date AS date#1461, data#1450.created_time AS created_time#1462]\n            :                 +- Project [from_json(StructField(latitude,StringType,true), StructField(longitude,DoubleType,true), StructField(air_temperature_celcius,IntegerType,true), StructField(relative_humidity,DoubleType,true), StructField(windspeed_knots,DoubleType,true), StructField(max_wind_speed,DoubleType,true), StructField(precipitation,StringType,true), StructField(GHI_w/m2,IntegerType,true), StructField(producer,StringType,true), StructField(date,StringType,true), StructField(created_time,IntegerType,true), cast(value#1437 as string), Some(Etc/UTC)) AS data#1450]\n            :                    +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@3189691b, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@63342902, [kafka.bootstrap.servers=172.16.55.43:9092, subscribe=A2_climate_topic], [key#1436, value#1437, topic#1438, partition#1439, offset#1440L, timestamp#1441, timestampType#1442], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4038c988,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 172.16.55.43:9092, subscribe -> A2_climate_topic),None), kafka, [key#1429, value#1430, topic#1431, partition#1432, offset#1433L, timestamp#1434, timestampType#1435]\n            +- SubqueryAlias hotspot\n               +- Project [latitude#3373, longitude#3374, confidence#3375, surface_temperature_celcius#3376, producer#3377, random_time#3378, climate_geohash(latitude#3373, longitude#3374)#3814 AS geohash#3815]\n                  +- Deduplicate [geohash#3806]\n                     +- Project [latitude#3373, longitude#3374, confidence#3375, surface_temperature_celcius#3376, producer#3377, random_time#3378, hotspot_geohash(latitude#3373, longitude#3374)#3805 AS geohash#3806]\n                        +- Project [data#3371.latitude AS latitude#3373, data#3371.longitude AS longitude#3374, data#3371.confidence AS confidence#3375, data#3371.surface_temperature_celcius AS surface_temperature_celcius#3376, data#3371.producer AS producer#3377, data#3371.random_time AS random_time#3378]\n                           +- Project [from_json(StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(confidence,DoubleType,true), StructField(surface_temperature_celcius,DoubleType,true), StructField(producer,StringType,true), StructField(random_time,IntegerType,true), cast(value#3358 as string), Some(Etc/UTC)) AS data#3371]\n                              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@5f228c81, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@33256074, [kafka.bootstrap.servers=172.16.55.43:9092, subscribe=A2_climate_topic], [key#3357, value#3358, topic#3359, partition#3360, offset#3361L, timestamp#3362, timestampType#3363], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4038c988,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 172.16.55.43:9092, subscribe -> A2_climate_topic),None), kafka, [key#3350, value#3351, topic#3352, partition#3353, offset#3354L, timestamp#3355, timestampType#3356]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterrupted by CTRL-C. Stopping query.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mquery\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d580c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349810c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3fd56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34318293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712a7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b1883a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89c6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff2857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7a481f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbWriter_climate:\n",
    "    \"\"\"\n",
    "    Check whether it works in pyspark log\n",
    "    \"\"\"\n",
    "    # called at the start of processing each partition in each output micro-batch\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.mongo_client = MongoClient(\n",
    "            host=f'{host_ip}',\n",
    "            port=27017\n",
    "        )\n",
    "        #use the same database, name: fit3182_db\n",
    "        self.db = self.mongo_client['fit3182_db']\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    # called once per row of the result dataframe\n",
    "    # the current code DOES NOT handle duplicate processing\n",
    "    #   e.g., query fails and restarts just before current micro-batch was fully inserted\n",
    "    def process(self, row):\n",
    "        #passing JSON string from row.value\n",
    "        #into a dictionary data\n",
    "        data = json.loads(row.value)\n",
    "        \n",
    "        db_record = {}\n",
    "        db_record['air_temperature_celcius'] = data.get('air_temperature_celcius')\n",
    "        db_record['relative_humidity'] = data.get('relative_humidity')\n",
    "        db_record['windspeed_knots'] = data.get('windspeed_knots')\n",
    "        db_record['max_wind_speed'] = data.get('max_wind_speed')\n",
    "        db_record['precipitation'] = data.get('precipitation')\n",
    "        db_record['GHI_w/m2'] = data.get('GHI_w/m2')\n",
    "        db_record['producer'] = data.get('producer')\n",
    "        db_record['date'] = data.get('date')\n",
    "        #print(db_record)\n",
    "        #print heren not working\n",
    "                  \n",
    "        #update and insert\n",
    "        # New database has nothing so just insert\n",
    "        # later something with the same station then update\n",
    "        self.db['A2'].replace_one({'station': data.get('kerbsideid')}, db_record, upsert=True)\n",
    "    \n",
    "    # called once all rows have been processed (possibly with error)\n",
    "    def close(self, err):\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08736cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = (\n",
    "    #initializes a streaming write for the parking_sdf DataFrame.\n",
    "    climate_sdf.writeStream.format(\"console\")\n",
    "    #output will be written to the standard console/output.\n",
    "    .option(\"checkpointLocation\", \"./climate_sdf_checkpoints\")\n",
    "    #Specifies the location for checkpointing, which allows streaming \n",
    "    #queries to be resilient to failures by storing the state.\n",
    "    #.outputMode('append'): Sets the output mode to ‘append’, meaning \n",
    "    #only new rows will be written to the output sink since the last trigger.\n",
    "    .outputMode('append').foreach(DbWriter_climate())\n",
    "    # Applies a foreach writer, which in this case is an instance of DbWriter(). \n",
    "    #This indicates that for each row in the output, the DbWriter() class’s process \n",
    "    #method will be called\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a14b396c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     query\u001b[38;5;241m.\u001b[39mawaitTermination()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/streaming.py:1389\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\nAggregate [geohash#1363], [geohash#1363, avg(surface_temperature_celcius#1422) AS avg(surface_temperature_celcius)#1568, avg(confidence#1421) AS avg(confidence)#1569]\n+- Project [geohash#1363, latitude#1342, longitude#1343, air_temperature_celcius#1344, relative_humidity#1345, windspeed_knots#1346, max_wind_speed#1347, precipitation#1348, GHI_w/m2#1349, producer#1350, date#1351, latitude#1419, longitude#1420, confidence#1421, surface_temperature_celcius#1422, created_time#1423, created_time#1424]\n   +- Join LeftOuter, (geohash#1363 = geohash#1524)\n      :- GlobalLimit 1\n      :  +- LocalLimit 1\n      :     +- Project [latitude#1342, longitude#1343, air_temperature_celcius#1344, relative_humidity#1345, windspeed_knots#1346, max_wind_speed#1347, precipitation#1348, GHI_w/m2#1349, producer#1350, date#1351, climate_geohash(latitude#1342, longitude#1343)#1362 AS geohash#1363]\n      :        +- Project [data#1340.latitude AS latitude#1342, data#1340.longitude AS longitude#1343, data#1340.air_temperature_celcius AS air_temperature_celcius#1344, data#1340.relative_humidity AS relative_humidity#1345, data#1340.windspeed_knots AS windspeed_knots#1346, data#1340.max_wind_speed AS max_wind_speed#1347, data#1340.precipitation AS precipitation#1348, data#1340.GHI_w/m2 AS GHI_w/m2#1349, data#1340.producer AS producer#1350, data#1340.date AS date#1351]\n      :           +- Project [from_json(StructField(latitude,StringType,true), StructField(longitude,DoubleType,true), StructField(air_temperature_celcius,IntegerType,true), StructField(relative_humidity,DoubleType,true), StructField(windspeed_knots,DoubleType,true), StructField(max_wind_speed,DoubleType,true), StructField(precipitation,StringType,true), StructField(GHI_w/m2,IntegerType,true), StructField(producer,StringType,true), StructField(date,StringType,true), cast(value#1327 as string), Some(Etc/UTC)) AS data#1340]\n      :              +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@6b840a63, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@485245e5, [kafka.bootstrap.servers=172.16.55.43:9092, subscribe=A2_climate_topic], [key#1326, value#1327, topic#1328, partition#1329, offset#1330L, timestamp#1331, timestampType#1332], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@30c9a569,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 172.16.55.43:9092, subscribe -> A2_climate_topic),None), kafka, [key#1319, value#1320, topic#1321, partition#1322, offset#1323L, timestamp#1324, timestampType#1325]\n      +- Project [latitude#1419, longitude#1420, confidence#1421, surface_temperature_celcius#1422, created_time#1423, created_time#1424, climate_geohash(latitude#1419, longitude#1420)#1523 AS geohash#1524]\n         +- Deduplicate [geohash#1515]\n            +- Project [latitude#1419, longitude#1420, confidence#1421, surface_temperature_celcius#1422, created_time#1423, created_time#1424, hotspot_geohash(latitude#1419, longitude#1420)#1514 AS geohash#1515]\n               +- Project [data#1417.latitude AS latitude#1419, data#1417.longitude AS longitude#1420, data#1417.confidence AS confidence#1421, data#1417.surface_temperature_celcius AS surface_temperature_celcius#1422, data#1417.created_time AS created_time#1423, data#1417.created_time AS created_time#1424]\n                  +- Project [from_json(StructField(latitude,DoubleType,true), StructField(longitude,DoubleType,true), StructField(confidence,DoubleType,true), StructField(surface_temperature_celcius,DoubleType,true), StructField(created_time,StringType,true), StructField(created_time,IntegerType,true), cast(value#1404 as string), Some(Etc/UTC)) AS data#1417]\n                     +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@25aa5511, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@735e7fe0, [kafka.bootstrap.servers=172.16.55.43:9092, subscribe=A2_climate_topic], [key#1403, value#1404, topic#1405, partition#1406, offset#1407L, timestamp#1408, timestampType#1409], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@30c9a569,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 172.16.55.43:9092, subscribe -> A2_climate_topic),None), kafka, [key#1396, value#1397, topic#1398, partition#1399, offset#1400L, timestamp#1401, timestampType#1402]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterrupted by CTRL-C. Stopping query.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mquery\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    query = writer.start()\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6aaf9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import from_json, col\n",
    "# climate_stream = (\n",
    "#     spark.readStream\n",
    "#     .format('kafka')\n",
    "#     .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "#     .option('subscribe', topic)\n",
    "#     #load into dataframe for raw data from source(Kafka)\n",
    "#     .load()    #parse JSON string into structured format (Dataframe) based on schema\n",
    "#     .select(from_json(col('value').cast('string'), climate_schema).alias('data'))\n",
    "#     .select('data.*')         \n",
    "# )           #cast bytes in value column into string\n",
    "#             #alias rename value column into data\n",
    "#             #flatten by expandign fields of the data column into separate columns\n",
    "#             #select data column since renamed\n",
    "            \n",
    "# hotspot_AQUA_stream = (\n",
    "#     spark.readStream \\\n",
    "#     .format(\"kafka\")\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \n",
    "#     .option(\"subscribe\", topic_AQUA)\n",
    "#     .load()\n",
    "#     .select(from_json(col(\"value\").cast(\"string\"), hotspot_schema).alias(\"data\"))\n",
    "#     .select(\"data.*\")\n",
    "# )\n",
    "\n",
    "# hotspot_TERRA_stream = (\n",
    "#     spark.readStream \\\n",
    "#     .format(\"kafka\")\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \n",
    "#     .option(\"subscribe\", topic_TERRA)\n",
    "#     .load()\n",
    "#     .select(from_json(col(\"value\").cast(\"string\"), hotspot_schema).alias(\"data\"))\n",
    "#     .select(\"data.*\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43fe3d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longtitude: double (nullable = true)\n",
      " |-- air_temperature_celcius: integer (nullable = true)\n",
      " |-- relative_humidity: double (nullable = true)\n",
      " |-- windspeed_knots: double (nullable = true)\n",
      " |-- max_wind_speed: double (nullable = true)\n",
      " |-- precipitation: string (nullable = true)\n",
      " |-- GHI_w/m2: integer (nullable = true)\n",
      " |-- producer: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longtitude: double (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- surface_temperature: double (nullable = true)\n",
      " |-- created_time: string (nullable = true)\n",
      " |-- created_time: integer (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longtitude: double (nullable = true)\n",
      " |-- confidence: double (nullable = true)\n",
      " |-- surface_temperature: double (nullable = true)\n",
      " |-- created_time: string (nullable = true)\n",
      " |-- created_time: integer (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m hotspot_TERRA_stream\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Show the contents of the DataFrame\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mclimate_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m hotspot_AQUA_stream\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     53\u001b[0m hotspot_TERRA_stream\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka"
     ]
    }
   ],
   "source": [
    "# import geohash\n",
    "# from pyspark.sql.functions import col, udf\n",
    "# from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# # Define the geohash UDF for hotspot\n",
    "# def hotspot_geohash(lat, lon):\n",
    "#     return geohash.encode(lat, lon, precision=5)\n",
    "\n",
    "# # Register the UDF\n",
    "# hotspot_geohash_udf = udf(hotspot_geohash, StringType())\n",
    "# spark.udf.register(\"hotspot_geohash\", hotspot_geohash_udf)\n",
    "\n",
    "\n",
    "# # Define the geohash UDF for climate\n",
    "# def climate_geohash(lat, lon):\n",
    "#     return geohash.encode(lat, lon, precision=3)\n",
    "\n",
    "# # # Register UDFs in Spark\n",
    "# climate_geohash_udf = udf(climate_geohash, StringType())\n",
    "# spark.udf.register(\"climate_geohash\", climate_geohash_udf)\n",
    "\n",
    "\n",
    "\n",
    "# # Apply the geohash UDF to the DataFrame columns\n",
    "# climate_stream = climate_stream.withColumn('geohash', climate_geohash_udf(col('latitude'), col('longtitude')))\n",
    "# hotspot_AQUA_stream = hotspot_AQUA_stream.withColumn('geohash', hotspot_geohash_udf(col('latitude'), col('longtitude')))\n",
    "# hotspot_TERRA_stream = hotspot_TERRA_stream.withColumn('geohash', hotspot_geohash_udf(col('latitude'), col('longtitude')))\n",
    "\n",
    "\n",
    "# # Join the streams and drop duplicates\n",
    "# joined_hotspot_stream = hotspot_AQUA_stream.join(hotspot_TERRA_stream, 'geohash').dropDuplicates(['geohash'])\n",
    "# final_stream = climate_stream.join(joined_hotspot_stream, 'geohash')\n",
    "\n",
    "\n",
    "# # # Assuming you have a joined DataFrame called 'joined_df1_df2'\n",
    "# # for row in joined_hotspot_stream.collect():\n",
    "# #     geohash = row[\"geohash\"]\n",
    "# #     temperature = row[\"temperature\"]\n",
    "# #     hotspot_count = row[\"hotspot_count\"]\n",
    "# #     # Store relevant information in the dictionary\n",
    "# #     result_dict[geohash] = {\"temperature\": temperature, \"hotspot_count\": hotspot_count}\n",
    "# #     print(row)\n",
    "# # # Now 'result_dict' contains the processed data\n",
    "# # Print the schema of the DataFrame\n",
    "# climate_stream.printSchema()\n",
    "# hotspot_AQUA_stream.printSchema()\n",
    "# hotspot_TERRA_stream.printSchema()\n",
    "\n",
    "# # Show the contents of the DataFrame\n",
    "# climate_stream.show()\n",
    "# hotspot_AQUA_stream.show()\n",
    "# hotspot_TERRA_stream.show()\n",
    "\n",
    "# # After joining the streams\n",
    "# joined_hotspot_stream.printSchema()\n",
    "# final_stream.printSchema()\n",
    "\n",
    "# # Show the contents after joining\n",
    "# joined_hotspot_stream.show()\n",
    "# final_stream.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130215c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
