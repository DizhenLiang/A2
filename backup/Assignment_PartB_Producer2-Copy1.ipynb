{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5b77fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/student/A2\n",
      "2024-01-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#change working directory\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir('/home/student/A2/drive-download-20240507T073745Z-001')\n",
    "\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "#Residence wifi ip address: 172.16.62.212\n",
    "ipadd = \"10.192.33.112\"\n",
    "\n",
    "\n",
    "client = MongoClient(ipadd, 27017)\n",
    "db = client.fit3182_db\n",
    "collection = db.fit3182_assignment_db\n",
    "\n",
    "#res = collection.find().sort('surface_temperature_celcius', pymongo.DESCENDING).limit(10)\n",
    "res = collection.find().sort('date', pymongo.DESCENDING).limit(1)\n",
    "\n",
    "for d in res:\n",
    "    res = d\n",
    "#     print(d)\n",
    "\n",
    "latest_date = res['date']\n",
    "#string to datetime object\n",
    "# latest_date = datetime.strptime(latest_date, '%Y-%m-%d %H:%M:%S')\n",
    "#string to datetime object\n",
    "latest_date = datetime.strptime(latest_date, '%d/%m/%Y')\n",
    "start_date = latest_date + timedelta(days=1)\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bcf634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka3 import KafkaProducer\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "hostip = ipadd # change it to your IP\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    producer = None\n",
    "    \n",
    "    try:\n",
    "        producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'],api_version=(0,10))\n",
    "    \n",
    "    except Exception as ex:\n",
    "        print(\"Exception while connecting Kafka\")\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return producer \n",
    "    \n",
    "def publish_msg(producer, topic, data):\n",
    "    try:\n",
    "        #print(data)        #send the  value by dumps to JSON object\n",
    "                            #then encode with utf-8 into bytes\n",
    "        value_bytes = json.dumps(data).encode('utf-8')\n",
    "        producer.send(topic, value=value_bytes)\n",
    "        #print(data)\n",
    "        \"\"\"\n",
    "        The flush() method in Kafkaâ€™s producer API is used to ensure that all \n",
    "        previously sent messages have been transmitted to the server and acknowledged \n",
    "        before proceeding. When you call flush(), it blocks the current thread and waits \n",
    "        for the producer to complete the sending of all records\n",
    "        \"\"\"\n",
    "        producer.flush()  \n",
    "    except Exception as ex:\n",
    "        print('Exception from publish_msg func')\n",
    "        print(str(ex))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b9b6e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n",
      "00:00:00\n",
      "00:01:00\n",
      "00:03:00\n",
      "00:06:00\n",
      "00:09:00\n",
      "00:12:00\n",
      "00:13:00\n",
      "00:15:00\n",
      "00:18:00\n",
      "00:19:00\n",
      "00:21:00\n",
      "00:24:00\n",
      "00:27:00\n",
      "00:30:00\n",
      "00:32:00\n",
      "00:34:00\n",
      "00:35:00\n",
      "00:36:00\n",
      "00:37:00\n",
      "00:39:00\n",
      "00:41:00\n",
      "00:43:00\n",
      "00:45:00\n",
      "00:47:00\n",
      "00:48:00\n",
      "00:50:00\n",
      "00:51:00\n",
      "00:54:00\n",
      "00:57:00\n",
      "00:58:00\n",
      "01:00:00\n",
      "01:02:00\n",
      "01:05:00\n",
      "01:06:00\n",
      "01:09:00\n",
      "01:12:00\n",
      "01:14:00\n",
      "01:16:00\n",
      "01:19:00\n",
      "01:20:00\n",
      "01:21:00\n",
      "01:23:00\n",
      "01:24:00\n",
      "01:26:00\n",
      "01:27:00\n",
      "01:29:00\n",
      "01:30:00\n",
      "01:33:00\n",
      "01:34:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m start_time \u001b[38;5;241m=\u001b[39m start_time \u001b[38;5;241m+\u001b[39m timedelta(minutes\u001b[38;5;241m=\u001b[39mrandom_time)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# every random seconds\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_time\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Event Producer 2\n",
    "\"\"\"\n",
    "hotspot_AQUA = pd.read_csv(\"./hotspot_AQUA_streaming.csv\", sep=',')\n",
    "\n",
    "hotspot_AQUA.head()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main Function\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    topic = 'Producer2'\n",
    "    print('Publishing records..')\n",
    "    producer02 = connect_kafka_producer()\n",
    "    \n",
    "    \n",
    "    rand_row_indices = list(range(len(hotspot_AQUA.index)))\n",
    "    random.shuffle(rand_row_indices) \n",
    "    # random choose time from 1 to 10\n",
    "    max_time = 3 #testing purpose\n",
    "    start_time = start_date\n",
    "    #rand_time = list(range(1,max_time))\n",
    "#     random.shuffle(rand_time) \n",
    "     \n",
    "\n",
    "    for index, hotspot_AQUA_row in hotspot_AQUA.iterrows():\n",
    "        random_time = rand_row_indices[index]%max_time + 1\n",
    "        \n",
    "        hotspot_AQUA_row = hotspot_AQUA.loc[rand_row_indices[index]]\n",
    "\n",
    "        hotspot_AQUA_doc = {\n",
    "            'latitude': hotspot_AQUA_row['latitude'],\n",
    "            'longitude': hotspot_AQUA_row['longitude'],\n",
    "            'confidence': hotspot_AQUA_row['confidence'],\n",
    "            'surface_temperature_celcius': hotspot_AQUA_row['surface_temperature_celcius'],\n",
    "            'producer': \"Producer_2\",\n",
    "            'created_time': start_time.strftime('%H:%M:%S')# at least 1 sec\n",
    "        }\n",
    "        #update start date\n",
    "        #print(climate_doc['date'])\n",
    "        #producer publish msg in strictured streaming\n",
    "        # convert the dictionary to JSON-formatted string and encode as UFT-8 by publish_msg\n",
    "        # msg = json.dumps(climate_doc)\n",
    "        print(hotspot_AQUA_doc['created_time'])\n",
    "        #print(hotspot_AQUA_doc['random_time'])\n",
    "        publish_msg(producer02, topic, hotspot_AQUA_doc)\n",
    "        #random minute for created_time\n",
    "        start_time = start_time + timedelta(minutes=random_time)\n",
    "        # every random seconds\n",
    "        sleep(random_time) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fe0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065caf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
